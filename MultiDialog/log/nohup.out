ssh://bureaux@172.30.2.148:22/home/bureaux/miniconda3/envs/Keras-base/bin/python -u /home/bureaux/.pycharm_helpers/pydev/pydevconsole.py --mode=server
import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/home/bureaux/Projects/keras4bert', '/home/bureaux/Projects/keras4bert'])
Python 3.6.13 |Anaconda, Inc.| (default, Jun  4 2021, 14:25:59)
Type 'copyright', 'credits' or 'license' for more information
IPython 7.16.1 -- An enhanced Interactive Python. Type '?' for help.
runfile('/home/bureaux/Projects/keras4bert/MultiDialog/train.py', wdir='/home/bureaux/Projects/keras4bert/MultiDialog')
PyDev console: using IPython 7.16.1
Python 3.6.13 |Anaconda, Inc.| (default, Jun  4 2021, 14:25:59)
[GCC 7.5.0] on linux
Using TensorFlow backend.
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 1.288 seconds.
Prefix dict has been built successfully.
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.
2022-07-22 16:41:15.702658: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2022-07-22 16:41:15.717951: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2022-07-22 16:41:16.158601: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5609ba2234c0 executing computations on platform CUDA. Devices:
2022-07-22 16:41:16.158671: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Graphics Device, Compute Capability 7.0
2022-07-22 16:41:16.163150: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2022-07-22 16:41:16.165810: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5609ba364260 executing computations on platform Host. Devices:
2022-07-22 16:41:16.165881: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2022-07-22 16:41:16.167808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: Graphics Device major: 7 minor: 0 memoryClockRate(GHz): 1.597
pciBusID: 0000:3b:00.0
2022-07-22 16:41:16.168304: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2022-07-22 16:41:16.170579: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2022-07-22 16:41:16.172750: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2022-07-22 16:41:16.173234: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2022-07-22 16:41:16.175887: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2022-07-22 16:41:16.177975: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2022-07-22 16:41:16.184231: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2022-07-22 16:41:16.188018: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2022-07-22 16:41:16.188159: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2022-07-22 16:41:16.190092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-07-22 16:41:16.190139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0
2022-07-22 16:41:16.190183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N
2022-07-22 16:41:16.193367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30458 MB memory) -> physical GPU (device: 0, name: Graphics Device, pci bus id: 0000:3b:00.0, compute capability: 7.0)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
Input-Token (InputLayer)        (None, None)         0
__________________________________________________________________________________________________
Input-Segment (InputLayer)      (None, None)         0
__________________________________________________________________________________________________
Embedding-Token (Embedding)     multiple             11248896    Input-Token[0][0]
                                                                 MLM-Norm[0][0]
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]
                                                                 Embedding-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Token-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]
__________________________________________________________________________________________________
Attention-LM-Mask (Lambda)      (1, 1, None, None)   0           Input-Token[0][0]
__________________________________________________________________________________________________
Embedding-Relative-Position (Re (None, None, 64)     8256        Embedding-Dropout[0][0]
                                                                 Embedding-Dropout[0][0]
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]
                                                                 Embedding-Dropout[0][0]
                                                                 Embedding-Dropout[0][0]
                                                                 Attention-LM-Mask[0][0]
                                                                 Embedding-Relative-Position[0][0]
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]
                                                                 Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward (Feed (None, None, 768)    4722432     Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward-Dropo (None, None, 768)    0           Transformer-0-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-0-FeedForward-Add ( (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
                                                                 Transformer-0-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-0-FeedForward-Norm  (None, None, 768)    1536        Transformer-0-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Attention-LM-Mask[0][0]
                                                                 Embedding-Relative-Position[0][0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Attention-LM-Mask[0][0]
                                                                 Embedding-Relative-Position[0][0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Attention-LM-Mask[0][0]
                                                                 Embedding-Relative-Position[0][0]
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 768)    4722432     Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 768)    1536        Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
                                                                 Attention-LM-Mask[0][0]
                                                                 Embedding-Relative-Position[0][0]
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward (Feed (None, None, 768)    4722432     Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
                                                                 Transformer-4-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-4-FeedForward-Norm  (None, None, 768)    1536        Transformer-4-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
                                                                 Attention-LM-Mask[0][0]
                                                                 Embedding-Relative-Position[0][0]
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward (Feed (None, None, 768)    4722432     Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
                                                                 Transformer-5-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-5-FeedForward-Norm  (None, None, 768)    1536        Transformer-5-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
                                                                 Attention-LM-Mask[0][0]
                                                                 Embedding-Relative-Position[0][0]
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward (Feed (None, None, 768)    4722432     Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward-Dropo (None, None, 768)    0           Transformer-6-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-6-FeedForward-Add ( (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
                                                                 Transformer-6-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-6-FeedForward-Norm  (None, None, 768)    1536        Transformer-6-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
                                                                 Attention-LM-Mask[0][0]
                                                                 Embedding-Relative-Position[0][0]
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward (Feed (None, None, 768)    4722432     Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward-Dropo (None, None, 768)    0           Transformer-7-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-7-FeedForward-Add ( (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
                                                                 Transformer-7-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-7-FeedForward-Norm  (None, None, 768)    1536        Transformer-7-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
                                                                 Attention-LM-Mask[0][0]
                                                                 Embedding-Relative-Position[0][0]
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward (Feed (None, None, 768)    4722432     Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward-Dropo (None, None, 768)    0           Transformer-8-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-8-FeedForward-Add ( (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
                                                                 Transformer-8-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-8-FeedForward-Norm  (None, None, 768)    1536        Transformer-8-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
                                                                 Attention-LM-Mask[0][0]
                                                                 Embedding-Relative-Position[0][0]
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward (Feed (None, None, 768)    4722432     Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward-Dropo (None, None, 768)    0           Transformer-9-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-9-FeedForward-Add ( (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
                                                                 Transformer-9-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-9-FeedForward-Norm  (None, None, 768)    1536        Transformer-9-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
                                                                 Attention-LM-Mask[0][0]
                                                                 Embedding-Relative-Position[0][0]
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward (Fee (None, None, 768)    4722432     Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward-Drop (None, None, 768)    0           Transformer-10-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-10-FeedForward-Add  (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
                                                                 Transformer-10-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-10-FeedForward-Norm (None, None, 768)    1536        Transformer-10-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
                                                                 Attention-LM-Mask[0][0]
                                                                 Embedding-Relative-Position[0][0]
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-FeedForward-Norm[0
                                                                 Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward (Fee (None, None, 768)    4722432     Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward-Drop (None, None, 768)    0           Transformer-11-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-11-FeedForward-Add  (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
                                                                 Transformer-11-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-11-FeedForward-Norm (None, None, 768)    1536        Transformer-11-FeedForward-Add[0]
__________________________________________________________________________________________________
MLM-Dense (Dense)               (None, None, 768)    590592      Transformer-11-FeedForward-Norm[0
__________________________________________________________________________________________________
MLM-Norm (LayerNormalization)   (None, None, 768)    1536        MLM-Dense[0][0]
__________________________________________________________________________________________________
MLM-Bias (ScaleOffset)          (None, None, 14647)  14647       Embedding-Token[1][0]
__________________________________________________________________________________________________
MLM-Activation (Activation)     (None, None, 14647)  0           MLM-Bias[0][0]
__________________________________________________________________________________________________
cross_entropy_1 (CrossEntropy)  (None, None, 14647)  0           Input-Token[0][0]
                                                                 MLM-Activation[0][0]
==================================================================================================
Total params: 96,921,463
Trainable params: 96,913,207
Non-trainable params: 8,256
__________________________________________________________________________________________________
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Epoch 1/9999
2022-07-22 16:42:38.380635: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
1000/1000 [==============================] - 522s 522ms/step - loss: 5.7867 - val_loss: 4.3942
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 你吃的好多的。
Epoch 2/9999
1000/1000 [==============================] - 493s 493ms/step - loss: 4.2599 - val_loss: 3.9953
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 不是我的，是甜的
Epoch 3/9999
1000/1000 [==============================] - 518s 518ms/step - loss: 4.0398 - val_loss: 3.8579
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 我不喜欢甜甜的，甜的都是甜的
Epoch 4/9999
1000/1000 [==============================] - 518s 518ms/step - loss: 3.9387 - val_loss: 3.7701
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 我也喜欢
Epoch 5/9999
1000/1000 [==============================] - 510s 510ms/step - loss: 3.8759 - val_loss: 3.7258
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 我喜欢甜甜的
Epoch 6/9999
1000/1000 [==============================] - 507s 507ms/step - loss: 3.8314 - val_loss: 3.6851
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 我喜欢甜的甜的
Epoch 7/9999
1000/1000 [==============================] - 508s 508ms/step - loss: 3.7938 - val_loss: 3.6669
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 哈哈哈哈哈哈哈哈哈哈哈
Epoch 8/9999
1000/1000 [==============================] - 513s 513ms/step - loss: 3.7827 - val_loss: 3.6379
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 我是喜欢甜的
Epoch 9/9999
1000/1000 [==============================] - 503s 503ms/step - loss: 3.7542 - val_loss: 3.6274
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 我喜欢甜
Epoch 10/9999
1000/1000 [==============================] - 505s 505ms/step - loss: 3.7345 - val_loss: 3.6099
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 我喜欢甜的
Epoch 11/9999
1000/1000 [==============================] - 505s 505ms/step - loss: 3.7169 - val_loss: 3.6000
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 我只喜欢甜的
Epoch 12/9999
1000/1000 [==============================] - 507s 507ms/step - loss: 3.7104 - val_loss: 3.5824
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 我也是，不过我喜欢甜的
Epoch 13/9999
1000/1000 [==============================] - 527s 527ms/step - loss: 3.6888 - val_loss: 3.5718
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 我是不喜欢甜的
Epoch 14/9999
1000/1000 [==============================] - 502s 502ms/step - loss: 3.6846 - val_loss: 3.5496
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 哈哈哈哈哈哈哈哈哈哈你也喜欢甜吗
Epoch 15/9999
1000/1000 [==============================] - 508s 508ms/step - loss: 3.6750 - val_loss: 3.5409
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 那你不是在甜的
Epoch 16/9999
1000/1000 [==============================] - 504s 504ms/step - loss: 3.6650 - val_loss: 3.5369
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 哈哈哈哈，我不喜欢哈哈哈
Epoch 17/9999
1000/1000 [==============================] - 515s 515ms/step - loss: 3.6592 - val_loss: 3.5275
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 哈哈哈哈，你吃的不甜
Epoch 18/9999
1000/1000 [==============================] - 506s 506ms/step - loss: 3.6466 - val_loss: 3.5169
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 好吧，我喜欢甜的
Epoch 19/9999
1000/1000 [==============================] - 504s 504ms/step - loss: 3.6370 - val_loss: 3.5185
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 你也是，我也喜欢甜
Epoch 20/9999
1000/1000 [==============================] - 510s 510ms/step - loss: 3.6307 - val_loss: 3.5049
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 我也是喜欢
Epoch 21/9999
1000/1000 [==============================] - 513s 513ms/step - loss: 3.6227 - val_loss: 3.5017
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 哈哈
Epoch 22/9999
1000/1000 [==============================] - 504s 504ms/step - loss: 3.6189 - val_loss: 3.4926
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 我也不喜欢甜的
Epoch 23/9999
1000/1000 [==============================] - 507s 507ms/step - loss: 3.6157 - val_loss: 3.4977
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 我也不喜欢吃甜的
Epoch 24/9999
1000/1000 [==============================] - 512s 512ms/step - loss: 3.6067 - val_loss: 3.4811
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 哈哈！！我不吃甜的
Epoch 25/9999
1000/1000 [==============================] - 509s 509ms/step - loss: 3.5927 - val_loss: 3.4785
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 哈哈哈哈哈你喜欢的是甜的！！
Epoch 26/9999
1000/1000 [==============================] - 494s 494ms/step - loss: 3.5898 - val_loss: 3.4762
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 我不喜欢
Epoch 27/9999
1000/1000 [==============================] - 514s 514ms/step - loss: 3.5871 - val_loss: 3.4677
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 我是不知道吃什么
Epoch 28/9999
1000/1000 [==============================] - 503s 503ms/step - loss: 3.5906 - val_loss: 3.4608
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 你吃过吗
Epoch 29/9999
1000/1000 [==============================] - 514s 514ms/step - loss: 3.5794 - val_loss: 3.4569
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 哈哈哈哈哈哈你不爱吃
Epoch 30/9999
1000/1000 [==============================] - 513s 513ms/step - loss: 3.5689 - val_loss: 3.4613
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 哈哈哈哈哈哈哈哈
Epoch 31/9999
1000/1000 [==============================] - 513s 513ms/step - loss: 3.5702 - val_loss: 3.4460
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 不是我喜欢的，是你不喜欢甜
Epoch 32/9999
1000/1000 [==============================] - 504s 504ms/step - loss: 3.5648 - val_loss: 3.4588
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 不要这样说
Epoch 33/9999
1000/1000 [==============================] - 517s 517ms/step - loss: 3.5562 - val_loss: 3.4363
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 我也不喜欢吃
Epoch 34/9999
1000/1000 [==============================] - 509s 509ms/step - loss: 3.5595 - val_loss: 3.4357
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 不喜欢吃甜的
Epoch 35/9999
1000/1000 [==============================] - 506s 506ms/step - loss: 3.5421 - val_loss: 3.4361
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 你喜欢甜的？
Epoch 36/9999
1000/1000 [==============================] - 507s 507ms/step - loss: 3.5468 - val_loss: 3.4284
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 哈哈哈哈哈哈，我是想吃
Epoch 37/9999
1000/1000 [==============================] - 510s 510ms/step - loss: 3.5398 - val_loss: 3.4281
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 我也不喜欢甜的，你喜欢不喜欢甜的？
Epoch 38/9999
1000/1000 [==============================] - 502s 502ms/step - loss: 3.5418 - val_loss: 3.4201
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 我喜欢甜的
Epoch 39/9999
1000/1000 [==============================] - 511s 511ms/step - loss: 3.5277 - val_loss: 3.4265
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 你也喜欢甜的？
Epoch 40/9999
1000/1000 [==============================] - 512s 512ms/step - loss: 3.5256 - val_loss: 3.4227
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 那你不喜欢我不喜欢吃甜
Epoch 41/9999
1000/1000 [==============================] - 503s 503ms/step - loss: 3.5283 - val_loss: 3.4139
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 我是喜欢甜的，甜的我就不吃
Epoch 42/9999
1000/1000 [==============================] - 499s 499ms/step - loss: 3.5192 - val_loss: 3.3999
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 那你吃甜的好吗？
Epoch 43/9999
1000/1000 [==============================] - 499s 499ms/step - loss: 3.5177 - val_loss: 3.4128
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 我不喜欢吃甜的
Epoch 44/9999
1000/1000 [==============================] - 504s 504ms/step - loss: 3.5184 - val_loss: 3.4228
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 那我也不好吃
Epoch 45/9999
1000/1000 [==============================] - 509s 509ms/step - loss: 3.5102 - val_loss: 3.4117
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 我喜欢吃甜的
Epoch 46/9999
1000/1000 [==============================] - 510s 510ms/step - loss: 3.5111 - val_loss: 3.4157
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 那就好
Epoch 47/9999
1000/1000 [==============================] - 510s 510ms/step - loss: 3.5000 - val_loss: 3.4016
dialog: ['我觉得巧克力好苦', '那是你没吃到甜的', '我不喜欢吃甜的']
answer: 我不喜欢
Epoch 00047: early stopping
进程已结束,退出代码0
