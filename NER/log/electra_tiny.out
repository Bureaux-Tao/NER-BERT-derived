ssh://bureaux@172.30.2.148:22/home/bureaux/miniconda3/envs/Keras-base/bin/python -u /home/bureaux/Projects/keras4bert/NER/train.py
Using TensorFlow backend.
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.

WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.

__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
Input-Token (InputLayer)        (None, None)         0
__________________________________________________________________________________________________
Input-Segment (InputLayer)      (None, None)         0
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 256)    5408768     Input-Token[0][0]
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 256)    512         Input-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 256)    0           Embedding-Token[0][0]
                                                                 Embedding-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 256)    131072      Embedding-Token-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 256)    512         Embedding-Position[0][0]
__________________________________________________________________________________________________
Embedding-Dropout (Dropout)     (None, None, 256)    0           Embedding-Norm[0][0]
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 256)    263168      Embedding-Dropout[0][0]
                                                                 Embedding-Dropout[0][0]
                                                                 Embedding-Dropout[0][0]
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 256)    0           Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 256)    0           Embedding-Dropout[0][0]
                                                                 Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 256)    512         Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward (Feed (None, None, 256)    525568      Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward-Dropo (None, None, 256)    0           Transformer-0-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-0-FeedForward-Add ( (None, None, 256)    0           Transformer-0-MultiHeadSelfAttent
                                                                 Transformer-0-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-0-FeedForward-Norm  (None, None, 256)    512         Transformer-0-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 256)    263168      Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 256)    0           Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 256)    0           Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 256)    512         Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 256)    525568      Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Dropo (None, None, 256)    0           Transformer-1-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 256)    0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 256)    512         Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 256)    263168      Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 256)    0           Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 256)    0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 256)    512         Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 256)    525568      Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Dropo (None, None, 256)    0           Transformer-2-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 256)    0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 256)    512         Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 256)    263168      Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 256)    0           Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 256)    0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 256)    512         Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 256)    525568      Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Dropo (None, None, 256)    0           Transformer-3-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 256)    0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 256)    512         Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 256)    263168      Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 256)    0           Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 256)    0           Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 256)    512         Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward (Feed (None, None, 256)    525568      Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward-Dropo (None, None, 256)    0           Transformer-4-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-4-FeedForward-Add ( (None, None, 256)    0           Transformer-4-MultiHeadSelfAttent
                                                                 Transformer-4-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-4-FeedForward-Norm  (None, None, 256)    512         Transformer-4-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 256)    263168      Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 256)    0           Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 256)    0           Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 256)    512         Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward (Feed (None, None, 256)    525568      Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward-Dropo (None, None, 256)    0           Transformer-5-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-5-FeedForward-Add ( (None, None, 256)    0           Transformer-5-MultiHeadSelfAttent
                                                                 Transformer-5-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-5-FeedForward-Norm  (None, None, 256)    512         Transformer-5-FeedForward-Add[0][
__________________________________________________________________________________________________
time_distributed_1 (TimeDistrib (None, None, 29)     7453        Transformer-5-FeedForward-Norm[0]
__________________________________________________________________________________________________
dropout_25 (Dropout)            (None, None, 29)     0           time_distributed_1[0][0]
__________________________________________________________________________________________________
conditional_random_field_1 (Con (None, None, 29)     841         dropout_25[0][0]
==================================================================================================
Total params: 10,287,718
Trainable params: 10,287,718
Non-trainable params: 0
__________________________________________________________________________________________________
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:2403: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Epoch 1/300
13/13 [==============================] - 14s 1s/step - loss: 43.1565 - sparse_accuracy: 0.2888
train:  f1: 0.60341, precision: 0.65596, recall: 0.55865
test:  f1: 0.60465, precision: 0.65000, recall: 0.56522

Epoch 00001: loss improved from inf to 43.66886, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 2/300
13/13 [==============================] - 3s 250ms/step - loss: 18.0875 - sparse_accuracy: 0.3356
train:  f1: 0.77007, precision: 0.77178, recall: 0.76836
test:  f1: 0.74304, precision: 0.75541, recall: 0.73108

Epoch 00002: loss improved from 43.66886 to 18.18916, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 3/300
13/13 [==============================] - 3s 249ms/step - loss: 12.7807 - sparse_accuracy: 0.4075
train:  f1: 0.78098, precision: 0.79349, recall: 0.76885
test:  f1: 0.71122, precision: 0.72927, recall: 0.69404

Epoch 00003: loss improved from 18.18916 to 12.76997, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 4/300
13/13 [==============================] - 3s 251ms/step - loss: 10.1918 - sparse_accuracy: 0.4570
train:  f1: 0.82567, precision: 0.82214, recall: 0.82923
test:  f1: 0.76699, precision: 0.77073, recall: 0.76329

Epoch 00004: loss improved from 12.76997 to 10.29415, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 5/300
13/13 [==============================] - 3s 227ms/step - loss: 9.3433 - sparse_accuracy: 0.5194
train:  f1: 0.85563, precision: 0.85468, recall: 0.85658
test:  f1: 0.78331, precision: 0.78080, recall: 0.78583

Epoch 00005: loss improved from 10.29415 to 9.32047, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 6/300
13/13 [==============================] - 3s 246ms/step - loss: 8.0644 - sparse_accuracy: 0.5346
train:  f1: 0.87768, precision: 0.87393, recall: 0.88147
test:  f1: 0.78406, precision: 0.77603, recall: 0.79227

Epoch 00006: loss improved from 9.32047 to 8.03566, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 7/300
13/13 [==============================] - 3s 235ms/step - loss: 7.6386 - sparse_accuracy: 0.5437
train:  f1: 0.88954, precision: 0.88704, recall: 0.89207
test:  f1: 0.79170, precision: 0.78481, recall: 0.79871

Epoch 00007: loss improved from 8.03566 to 7.57074, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 8/300
13/13 [==============================] - 3s 247ms/step - loss: 7.0217 - sparse_accuracy: 0.5635
train:  f1: 0.89036, precision: 0.88818, recall: 0.89256
test:  f1: 0.81315, precision: 0.80990, recall: 0.81643

Epoch 00008: loss improved from 7.57074 to 7.05834, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 9/300
13/13 [==============================] - 3s 248ms/step - loss: 6.2755 - sparse_accuracy: 0.5860
train:  f1: 0.92556, precision: 0.92579, recall: 0.92533
test:  f1: 0.78018, precision: 0.77460, recall: 0.78583

Epoch 00009: loss improved from 7.05834 to 6.25781, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 10/300
13/13 [==============================] - 3s 268ms/step - loss: 5.8060 - sparse_accuracy: 0.5969
train:  f1: 0.93115, precision: 0.93081, recall: 0.93149
test:  f1: 0.81296, precision: 0.81759, recall: 0.80837

Epoch 00010: loss improved from 6.25781 to 5.79648, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 11/300
13/13 [==============================] - 3s 249ms/step - loss: 5.7338 - sparse_accuracy: 0.6074
train:  f1: 0.93292, precision: 0.93710, recall: 0.92878
test:  f1: 0.80257, precision: 0.80000, recall: 0.80515

Epoch 00011: loss improved from 5.79648 to 5.71050, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 12/300
13/13 [==============================] - 3s 232ms/step - loss: 5.7011 - sparse_accuracy: 0.6275
train:  f1: 0.94209, precision: 0.94407, recall: 0.94012
test:  f1: 0.80095, precision: 0.78906, recall: 0.81320

Epoch 00012: loss improved from 5.71050 to 5.66525, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 13/300
13/13 [==============================] - 3s 226ms/step - loss: 5.4471 - sparse_accuracy: 0.6295
train:  f1: 0.95757, precision: 0.95852, recall: 0.95663
test:  f1: 0.79745, precision: 0.78989, recall: 0.80515

Epoch 00013: loss improved from 5.66525 to 5.39687, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 14/300
13/13 [==============================] - 3s 224ms/step - loss: 5.2804 - sparse_accuracy: 0.6417
train:  f1: 0.96012, precision: 0.96214, recall: 0.95811
test:  f1: 0.80319, precision: 0.79495, recall: 0.81159

Epoch 00014: loss improved from 5.39687 to 5.27073, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 15/300
13/13 [==============================] - 3s 242ms/step - loss: 4.9476 - sparse_accuracy: 0.6474
train:  f1: 0.95962, precision: 0.96164, recall: 0.95761
test:  f1: 0.81978, precision: 0.81201, recall: 0.82770

Epoch 00015: loss improved from 5.27073 to 4.95811, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 16/300
13/13 [==============================] - 3s 243ms/step - loss: 4.7883 - sparse_accuracy: 0.6502
train:  f1: 0.95053, precision: 0.95407, recall: 0.94702
test:  f1: 0.79421, precision: 0.79294, recall: 0.79549

Epoch 00016: loss improved from 4.95811 to 4.82579, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 17/300
13/13 [==============================] - 3s 242ms/step - loss: 4.8856 - sparse_accuracy: 0.6529
train:  f1: 0.96199, precision: 0.96666, recall: 0.95737
test:  f1: 0.79514, precision: 0.79967, recall: 0.79066

Epoch 00017: loss did not improve from 4.82579
Epoch 18/300
13/13 [==============================] - 3s 246ms/step - loss: 4.7611 - sparse_accuracy: 0.6634
train:  f1: 0.96357, precision: 0.96583, recall: 0.96131
test:  f1: 0.79712, precision: 0.79081, recall: 0.80354

Epoch 00018: loss improved from 4.82579 to 4.76903, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 19/300
13/13 [==============================] - 3s 238ms/step - loss: 4.6476 - sparse_accuracy: 0.6594
train:  f1: 0.96797, precision: 0.96773, recall: 0.96821
test:  f1: 0.80319, precision: 0.79495, recall: 0.81159

Epoch 00019: loss improved from 4.76903 to 4.67841, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 20/300
13/13 [==============================] - 3s 236ms/step - loss: 4.3488 - sparse_accuracy: 0.6684
train:  f1: 0.96009, precision: 0.96283, recall: 0.95737
test:  f1: 0.79428, precision: 0.78370, recall: 0.80515

Epoch 00020: loss improved from 4.67841 to 4.34901, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 21/300
13/13 [==============================] - 3s 232ms/step - loss: 4.9628 - sparse_accuracy: 0.6640
train:  f1: 0.96564, precision: 0.96505, recall: 0.96624
test:  f1: 0.79305, precision: 0.77829, recall: 0.80837

Epoch 00021: loss did not improve from 4.34901
Epoch 22/300
13/13 [==============================] - 3s 238ms/step - loss: 4.4185 - sparse_accuracy: 0.6661
train:  f1: 0.97016, precision: 0.97088, recall: 0.96944
test:  f1: 0.80381, precision: 0.79310, recall: 0.81481

Epoch 00022: loss did not improve from 4.34901
Epoch 23/300
13/13 [==============================] - 3s 226ms/step - loss: 4.5526 - sparse_accuracy: 0.6745
train:  f1: 0.97266, precision: 0.97218, recall: 0.97314
test:  f1: 0.79272, precision: 0.77916, recall: 0.80676

Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.

Epoch 00023: loss did not improve from 4.34901
Epoch 24/300
13/13 [==============================] - 3s 198ms/step - loss: 3.9988 - sparse_accuracy: 0.6789
train:  f1: 0.97559, precision: 0.97631, recall: 0.97486
test:  f1: 0.79491, precision: 0.78493, recall: 0.80515

Epoch 00024: loss improved from 4.34901 to 3.99125, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 25/300
13/13 [==============================] - 3s 227ms/step - loss: 3.7900 - sparse_accuracy: 0.6850
train:  f1: 0.97916, precision: 0.97977, recall: 0.97856
test:  f1: 0.79811, precision: 0.78207, recall: 0.81481

Epoch 00025: loss improved from 3.99125 to 3.76673, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 26/300
13/13 [==============================] - 3s 227ms/step - loss: 3.6429 - sparse_accuracy: 0.6875
train:  f1: 0.98028, precision: 0.98076, recall: 0.97979
test:  f1: 0.79874, precision: 0.78328, recall: 0.81481

Epoch 00026: loss improved from 3.76673 to 3.66688, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 27/300
13/13 [==============================] - 3s 233ms/step - loss: 3.6412 - sparse_accuracy: 0.6950
train:  f1: 0.98387, precision: 0.98350, recall: 0.98423
test:  f1: 0.81581, precision: 0.80124, recall: 0.83092

Epoch 00027: loss improved from 3.66688 to 3.63666, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 28/300
13/13 [==============================] - 3s 244ms/step - loss: 3.6210 - sparse_accuracy: 0.6933
train:  f1: 0.98349, precision: 0.98373, recall: 0.98324
test:  f1: 0.79717, precision: 0.77880, recall: 0.81643

Epoch 00028: loss improved from 3.63666 to 3.59458, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 29/300
13/13 [==============================] - 3s 243ms/step - loss: 3.5191 - sparse_accuracy: 0.6964
train:  f1: 0.98234, precision: 0.98465, recall: 0.98004
test:  f1: 0.80666, precision: 0.79407, recall: 0.81965

Epoch 00029: loss improved from 3.59458 to 3.49791, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 30/300
13/13 [==============================] - 3s 220ms/step - loss: 3.5887 - sparse_accuracy: 0.6995
train:  f1: 0.98323, precision: 0.98396, recall: 0.98250
test:  f1: 0.80380, precision: 0.79005, recall: 0.81804

Epoch 00030: loss did not improve from 3.49791
Epoch 31/300
13/13 [==============================] - 3s 230ms/step - loss: 3.4096 - sparse_accuracy: 0.6979
train:  f1: 0.98336, precision: 0.98349, recall: 0.98324
test:  f1: 0.81013, precision: 0.79627, recall: 0.82448

Epoch 00031: loss improved from 3.49791 to 3.40683, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 32/300
13/13 [==============================] - 3s 227ms/step - loss: 3.4921 - sparse_accuracy: 0.6984
train:  f1: 0.98534, precision: 0.98498, recall: 0.98571
test:  f1: 0.81581, precision: 0.80124, recall: 0.83092

Epoch 00032: loss did not improve from 3.40683
Epoch 33/300
13/13 [==============================] - 3s 238ms/step - loss: 3.2793 - sparse_accuracy: 0.6986
train:  f1: 0.98536, precision: 0.98403, recall: 0.98669
test:  f1: 0.80630, precision: 0.78891, recall: 0.82448

Epoch 00033: loss improved from 3.40683 to 3.29353, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 34/300
13/13 [==============================] - 3s 234ms/step - loss: 3.3320 - sparse_accuracy: 0.7047
train:  f1: 0.98559, precision: 0.98546, recall: 0.98571
test:  f1: 0.81077, precision: 0.79751, recall: 0.82448

Epoch 00034: loss did not improve from 3.29353
Epoch 35/300
13/13 [==============================] - 3s 239ms/step - loss: 3.1919 - sparse_accuracy: 0.7056
train:  f1: 0.98387, precision: 0.98350, recall: 0.98423
test:  f1: 0.80885, precision: 0.79380, recall: 0.82448

Epoch 00035: loss improved from 3.29353 to 3.20066, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 36/300
13/13 [==============================] - 3s 228ms/step - loss: 3.2662 - sparse_accuracy: 0.6995
train:  f1: 0.98224, precision: 0.98321, recall: 0.98127
test:  f1: 0.80094, precision: 0.78308, recall: 0.81965

Epoch 00036: loss did not improve from 3.20066
Epoch 37/300
13/13 [==============================] - 3s 227ms/step - loss: 3.4628 - sparse_accuracy: 0.7036
train:  f1: 0.98559, precision: 0.98546, recall: 0.98571
test:  f1: 0.80983, precision: 0.79719, recall: 0.82287

Epoch 00037: loss did not improve from 3.20066
Epoch 38/300
13/13 [==============================] - 3s 212ms/step - loss: 3.3422 - sparse_accuracy: 0.7055
train:  f1: 0.98261, precision: 0.98370, recall: 0.98152
test:  f1: 0.80569, precision: 0.79070, recall: 0.82126

Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.

Epoch 00038: loss did not improve from 3.20066
Epoch 39/300
13/13 [==============================] - 3s 230ms/step - loss: 3.2074 - sparse_accuracy: 0.7038
train:  f1: 0.98559, precision: 0.98523, recall: 0.98595
test:  f1: 0.81324, precision: 0.79630, recall: 0.83092

Epoch 00039: loss did not improve from 3.20066
Epoch 40/300
13/13 [==============================] - 3s 217ms/step - loss: 3.1373 - sparse_accuracy: 0.6995
train:  f1: 0.98372, precision: 0.98445, recall: 0.98300
test:  f1: 0.81137, precision: 0.79567, recall: 0.82770

Epoch 00040: loss improved from 3.20066 to 3.14244, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 41/300
13/13 [==============================] - 3s 225ms/step - loss: 3.1610 - sparse_accuracy: 0.7091
train:  f1: 0.98536, precision: 0.98403, recall: 0.98669
test:  f1: 0.81675, precision: 0.80155, recall: 0.83253

Epoch 00041: loss did not improve from 3.14244
Epoch 42/300
13/13 [==============================] - 3s 244ms/step - loss: 3.2101 - sparse_accuracy: 0.7076
train:  f1: 0.98584, precision: 0.98499, recall: 0.98669
test:  f1: 0.81329, precision: 0.79938, recall: 0.82770

Epoch 00042: loss did not improve from 3.14244
Epoch 43/300
13/13 [==============================] - 3s 214ms/step - loss: 3.1269 - sparse_accuracy: 0.7074
train:  f1: 0.98595, precision: 0.98595, recall: 0.98595
test:  f1: 0.81077, precision: 0.79751, recall: 0.82448

Epoch 00043: loss improved from 3.14244 to 3.12998, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 44/300
13/13 [==============================] - 3s 231ms/step - loss: 3.0789 - sparse_accuracy: 0.7061
train:  f1: 0.98534, precision: 0.98546, recall: 0.98521
test:  f1: 0.81201, precision: 0.79690, recall: 0.82770

Epoch 00044: loss improved from 3.12998 to 3.11070, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 45/300
13/13 [==============================] - 3s 220ms/step - loss: 3.1042 - sparse_accuracy: 0.7058
train:  f1: 0.98559, precision: 0.98523, recall: 0.98595
test:  f1: 0.80945, precision: 0.79199, recall: 0.82770

Epoch 00045: loss did not improve from 3.11070
Epoch 46/300
13/13 [==============================] - 3s 240ms/step - loss: 3.1056 - sparse_accuracy: 0.7085
train:  f1: 0.98633, precision: 0.98596, recall: 0.98669
test:  f1: 0.80094, precision: 0.78308, recall: 0.81965

Epoch 00046: loss did not improve from 3.11070
Epoch 47/300
13/13 [==============================] - 3s 224ms/step - loss: 3.1168 - sparse_accuracy: 0.7164
train:  f1: 0.98596, precision: 0.98571, recall: 0.98620
test:  f1: 0.80693, precision: 0.79012, recall: 0.82448

Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.

Epoch 00047: loss did not improve from 3.11070
Epoch 48/300
13/13 [==============================] - 3s 223ms/step - loss: 3.0813 - sparse_accuracy: 0.7123
train:  f1: 0.98571, precision: 0.98523, recall: 0.98620
test:  f1: 0.81552, precision: 0.80218, recall: 0.82931

Epoch 00048: loss improved from 3.11070 to 3.06942, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 49/300
13/13 [==============================] - 3s 219ms/step - loss: 2.9165 - sparse_accuracy: 0.7130
train:  f1: 0.98595, precision: 0.98619, recall: 0.98571
test:  f1: 0.80727, precision: 0.79225, recall: 0.82287

Epoch 00049: loss improved from 3.06942 to 2.91820, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 50/300
13/13 [==============================] - 3s 225ms/step - loss: 2.8947 - sparse_accuracy: 0.7115
train:  f1: 0.98374, precision: 0.98326, recall: 0.98423
test:  f1: 0.80663, precision: 0.79102, recall: 0.82287

Epoch 00050: loss improved from 2.91820 to 2.91709, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 51/300
13/13 [==============================] - 3s 225ms/step - loss: 3.0193 - sparse_accuracy: 0.7138
train:  f1: 0.98621, precision: 0.98548, recall: 0.98694
test:  f1: 0.80666, precision: 0.79407, recall: 0.81965

Epoch 00051: loss did not improve from 2.91709
Epoch 52/300
13/13 [==============================] - 3s 241ms/step - loss: 3.1149 - sparse_accuracy: 0.7138
train:  f1: 0.98633, precision: 0.98596, recall: 0.98669
test:  f1: 0.81043, precision: 0.79535, recall: 0.82609

Epoch 00052: loss did not improve from 2.91709
Epoch 53/300
13/13 [==============================] - 3s 241ms/step - loss: 2.9141 - sparse_accuracy: 0.7148
train:  f1: 0.98470, precision: 0.98592, recall: 0.98349
test:  f1: 0.81616, precision: 0.80343, recall: 0.82931

Epoch 00053: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.

Epoch 00053: loss did not improve from 2.91709
Epoch 54/300
13/13 [==============================] - 3s 227ms/step - loss: 2.9155 - sparse_accuracy: 0.7098
train:  f1: 0.98657, precision: 0.98645, recall: 0.98669
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00054: loss did not improve from 2.91709
Epoch 55/300
13/13 [==============================] - 3s 227ms/step - loss: 2.9056 - sparse_accuracy: 0.7108
train:  f1: 0.98657, precision: 0.98669, recall: 0.98645
test:  f1: 0.81423, precision: 0.79969, recall: 0.82931

Epoch 00055: loss improved from 2.91709 to 2.90671, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 56/300
13/13 [==============================] - 3s 220ms/step - loss: 2.8823 - sparse_accuracy: 0.7124
train:  f1: 0.98558, precision: 0.98594, recall: 0.98521
test:  f1: 0.81675, precision: 0.80155, recall: 0.83253

Epoch 00056: loss improved from 2.90671 to 2.86840, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 57/300
13/13 [==============================] - 3s 225ms/step - loss: 2.9857 - sparse_accuracy: 0.7092
train:  f1: 0.98632, precision: 0.98668, recall: 0.98595
test:  f1: 0.81962, precision: 0.80560, recall: 0.83414

Epoch 00057: loss did not improve from 2.86840
Epoch 58/300
13/13 [==============================] - 3s 242ms/step - loss: 2.8484 - sparse_accuracy: 0.7142
train:  f1: 0.98657, precision: 0.98669, recall: 0.98645
test:  f1: 0.81833, precision: 0.80310, recall: 0.83414

Epoch 00058: loss improved from 2.86840 to 2.84753, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 59/300
13/13 [==============================] - 3s 204ms/step - loss: 2.9777 - sparse_accuracy: 0.7113
train:  f1: 0.98632, precision: 0.98668, recall: 0.98595
test:  f1: 0.81423, precision: 0.79969, recall: 0.82931

Epoch 00059: loss did not improve from 2.84753
Epoch 60/300
13/13 [==============================] - 3s 207ms/step - loss: 2.9096 - sparse_accuracy: 0.7098
train:  f1: 0.98632, precision: 0.98668, recall: 0.98595
test:  f1: 0.81487, precision: 0.80093, recall: 0.82931

Epoch 00060: loss did not improve from 2.84753
Epoch 61/300
13/13 [==============================] - 3s 231ms/step - loss: 3.0651 - sparse_accuracy: 0.7145
train:  f1: 0.98632, precision: 0.98668, recall: 0.98595
test:  f1: 0.81487, precision: 0.80093, recall: 0.82931

Epoch 00061: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.

Epoch 00061: loss did not improve from 2.84753
Epoch 62/300
13/13 [==============================] - 3s 216ms/step - loss: 2.9312 - sparse_accuracy: 0.7144
train:  f1: 0.98632, precision: 0.98668, recall: 0.98595
test:  f1: 0.81681, precision: 0.80469, recall: 0.82931

Epoch 00062: loss did not improve from 2.84753
Epoch 63/300
13/13 [==============================] - 3s 228ms/step - loss: 2.9363 - sparse_accuracy: 0.7113
train:  f1: 0.98632, precision: 0.98668, recall: 0.98595
test:  f1: 0.81487, precision: 0.80093, recall: 0.82931

Epoch 00063: loss did not improve from 2.84753
Epoch 64/300
13/13 [==============================] - 3s 237ms/step - loss: 2.7946 - sparse_accuracy: 0.7159
train:  f1: 0.98632, precision: 0.98668, recall: 0.98595
test:  f1: 0.81423, precision: 0.79969, recall: 0.82931

Epoch 00064: loss improved from 2.84753 to 2.81772, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 65/300
13/13 [==============================] - 3s 259ms/step - loss: 2.9039 - sparse_accuracy: 0.7179
train:  f1: 0.98632, precision: 0.98668, recall: 0.98595
test:  f1: 0.81552, precision: 0.80218, recall: 0.82931

Epoch 00065: loss did not improve from 2.81772
Epoch 66/300
13/13 [==============================] - 3s 261ms/step - loss: 2.8354 - sparse_accuracy: 0.7085
train:  f1: 0.98632, precision: 0.98668, recall: 0.98595
test:  f1: 0.81552, precision: 0.80218, recall: 0.82931

Epoch 00066: loss did not improve from 2.81772
Epoch 67/300
13/13 [==============================] - 3s 231ms/step - loss: 2.7924 - sparse_accuracy: 0.7161
train:  f1: 0.98595, precision: 0.98643, recall: 0.98546
test:  f1: 0.81710, precision: 0.80374, recall: 0.83092

Epoch 00067: loss improved from 2.81772 to 2.79956, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_tiny_ner_crf.h5
Epoch 68/300
13/13 [==============================] - 3s 224ms/step - loss: 2.7880 - sparse_accuracy: 0.7153
train:  f1: 0.98595, precision: 0.98643, recall: 0.98546
test:  f1: 0.81681, precision: 0.80469, recall: 0.82931

Epoch 00068: loss did not improve from 2.79956
Epoch 69/300
13/13 [==============================] - 3s 220ms/step - loss: 3.0117 - sparse_accuracy: 0.7131
train:  f1: 0.98620, precision: 0.98644, recall: 0.98595
test:  f1: 0.81487, precision: 0.80093, recall: 0.82931

Epoch 00069: loss did not improve from 2.79956
Epoch 70/300
13/13 [==============================] - 3s 229ms/step - loss: 2.9183 - sparse_accuracy: 0.7140
train:  f1: 0.98620, precision: 0.98644, recall: 0.98595
test:  f1: 0.81329, precision: 0.79938, recall: 0.82770

Epoch 00070: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.

Epoch 00070: loss did not improve from 2.79956
Epoch 71/300
13/13 [==============================] - 3s 202ms/step - loss: 2.9491 - sparse_accuracy: 0.7110
train:  f1: 0.98595, precision: 0.98643, recall: 0.98546
test:  f1: 0.81329, precision: 0.79938, recall: 0.82770

Epoch 00071: loss did not improve from 2.79956
Epoch 72/300
13/13 [==============================] - 3s 242ms/step - loss: 2.9466 - sparse_accuracy: 0.7143
train:  f1: 0.98594, precision: 0.98667, recall: 0.98521
test:  f1: 0.81077, precision: 0.79751, recall: 0.82448

Epoch 00072: loss did not improve from 2.79956
Epoch 73/300
13/13 [==============================] - 3s 236ms/step - loss: 3.0892 - sparse_accuracy: 0.7118
train:  f1: 0.98631, precision: 0.98692, recall: 0.98571
test:  f1: 0.81329, precision: 0.79938, recall: 0.82770

Epoch 00073: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.

Epoch 00073: loss did not improve from 2.79956
Epoch 74/300
13/13 [==============================] - 3s 236ms/step - loss: 3.0469 - sparse_accuracy: 0.7140
train:  f1: 0.98594, precision: 0.98691, recall: 0.98497
test:  f1: 0.81077, precision: 0.79751, recall: 0.82448

Epoch 00074: loss did not improve from 2.79956
Epoch 75/300
13/13 [==============================] - 3s 242ms/step - loss: 2.9233 - sparse_accuracy: 0.7155
train:  f1: 0.98631, precision: 0.98692, recall: 0.98571
test:  f1: 0.81329, precision: 0.79938, recall: 0.82770

Epoch 00075: loss did not improve from 2.79956
Epoch 76/300
13/13 [==============================] - 3s 232ms/step - loss: 2.9707 - sparse_accuracy: 0.7119
train:  f1: 0.98594, precision: 0.98667, recall: 0.98521
test:  f1: 0.81077, precision: 0.79751, recall: 0.82448

Epoch 00076: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.

Epoch 00076: loss did not improve from 2.79956
Epoch 77/300
13/13 [==============================] - 3s 226ms/step - loss: 2.8528 - sparse_accuracy: 0.7135
train:  f1: 0.98594, precision: 0.98691, recall: 0.98497
test:  f1: 0.81077, precision: 0.79751, recall: 0.82448

Epoch 00077: loss did not improve from 2.79956
Epoch 00077: early stopping

进程已结束,退出代码0
