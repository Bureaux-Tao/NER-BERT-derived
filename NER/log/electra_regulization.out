ssh://bureaux@172.30.2.148:22/home/bureaux/miniconda3/envs/Keras-base/bin/python -u /home/bureaux/Projects/keras4bert/NER/train.py
Using TensorFlow backend.
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.

WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.

WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
Input-Token (InputLayer)        (None, None)         0
__________________________________________________________________________________________________
Input-Segment (InputLayer)      (None, None)         0
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 768)    16226304    Input-Token[0][0]
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]
                                                                 Embedding-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Position[0][0]
__________________________________________________________________________________________________
Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]
                                                                 Embedding-Dropout[0][0]
                                                                 Embedding-Dropout[0][0]
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]
                                                                 Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward (Feed (None, None, 768)    4722432     Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward-Dropo (None, None, 768)    0           Transformer-0-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-0-FeedForward-Add ( (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
                                                                 Transformer-0-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-0-FeedForward-Norm  (None, None, 768)    1536        Transformer-0-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 768)    4722432     Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 768)    1536        Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward (Feed (None, None, 768)    4722432     Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
                                                                 Transformer-4-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-4-FeedForward-Norm  (None, None, 768)    1536        Transformer-4-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward (Feed (None, None, 768)    4722432     Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
                                                                 Transformer-5-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-5-FeedForward-Norm  (None, None, 768)    1536        Transformer-5-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward (Feed (None, None, 768)    4722432     Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward-Dropo (None, None, 768)    0           Transformer-6-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-6-FeedForward-Add ( (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
                                                                 Transformer-6-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-6-FeedForward-Norm  (None, None, 768)    1536        Transformer-6-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward (Feed (None, None, 768)    4722432     Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward-Dropo (None, None, 768)    0           Transformer-7-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-7-FeedForward-Add ( (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
                                                                 Transformer-7-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-7-FeedForward-Norm  (None, None, 768)    1536        Transformer-7-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward (Feed (None, None, 768)    4722432     Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward-Dropo (None, None, 768)    0           Transformer-8-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-8-FeedForward-Add ( (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
                                                                 Transformer-8-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-8-FeedForward-Norm  (None, None, 768)    1536        Transformer-8-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward (Feed (None, None, 768)    4722432     Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward-Dropo (None, None, 768)    0           Transformer-9-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-9-FeedForward-Add ( (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
                                                                 Transformer-9-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-9-FeedForward-Norm  (None, None, 768)    1536        Transformer-9-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward (Fee (None, None, 768)    4722432     Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward-Drop (None, None, 768)    0           Transformer-10-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-10-FeedForward-Add  (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
                                                                 Transformer-10-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-10-FeedForward-Norm (None, None, 768)    1536        Transformer-10-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-FeedForward-Norm[0
                                                                 Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward (Fee (None, None, 768)    4722432     Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward-Drop (None, None, 768)    0           Transformer-11-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-11-FeedForward-Add  (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
                                                                 Transformer-11-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-11-FeedForward-Norm (None, None, 768)    1536        Transformer-11-FeedForward-Add[0]
__________________________________________________________________________________________________
bidirectional_1 (Bidirectional) (None, None, 128)    426496      Transformer-11-FeedForward-Norm[0
__________________________________________________________________________________________________
time_distributed_1 (TimeDistrib (None, None, 29)     3741        bidirectional_1[0][0]
__________________________________________________________________________________________________
dropout_13 (Dropout)            (None, None, 29)     0           time_distributed_1[0][0]
__________________________________________________________________________________________________
conditional_random_field_1 (Con (None, None, 29)     841         dropout_13[0][0]
==================================================================================================
Total params: 102,108,134
Trainable params: 102,108,134
Non-trainable params: 0
__________________________________________________________________________________________________
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

Epoch 1/300
13/13 [==============================] - 36s 3s/step - loss: 60.8133 - sparse_accuracy: 0.3908
test:  f1: 0.24575, precision: 0.29748, recall: 0.20934

Epoch 00001: loss improved from inf to 61.38282, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 2/300
13/13 [==============================] - 18s 1s/step - loss: 26.3702 - sparse_accuracy: 0.6092
test:  f1: 0.63308, precision: 0.66140, recall: 0.60709

Epoch 00002: loss improved from 61.38282 to 26.50517, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 3/300
13/13 [==============================] - 16s 1s/step - loss: 19.1113 - sparse_accuracy: 0.6593
test:  f1: 0.68538, precision: 0.70339, recall: 0.66828

Epoch 00003: loss improved from 26.50517 to 19.05716, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 4/300
13/13 [==============================] - 16s 1s/step - loss: 16.0183 - sparse_accuracy: 0.6832
test:  f1: 0.73804, precision: 0.74346, recall: 0.73269

Epoch 00004: loss improved from 19.05716 to 16.08387, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 5/300
13/13 [==============================] - 18s 1s/step - loss: 14.4284 - sparse_accuracy: 0.7096
test:  f1: 0.76973, precision: 0.76973, recall: 0.76973

Epoch 00005: loss improved from 16.08387 to 14.40997, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 6/300
13/13 [==============================] - 17s 1s/step - loss: 13.3292 - sparse_accuracy: 0.7181
test:  f1: 0.76314, precision: 0.76623, recall: 0.76006

Epoch 00006: loss improved from 14.40997 to 13.36158, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 7/300
13/13 [==============================] - 17s 1s/step - loss: 12.6373 - sparse_accuracy: 0.7232
test:  f1: 0.77985, precision: 0.78689, recall: 0.77295

Epoch 00007: loss improved from 13.36158 to 12.58593, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 8/300
13/13 [==============================] - 18s 1s/step - loss: 11.9951 - sparse_accuracy: 0.7257
test:  f1: 0.79321, precision: 0.79579, recall: 0.79066

Epoch 00008: loss improved from 12.58593 to 11.97436, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 9/300
13/13 [==============================] - 16s 1s/step - loss: 11.4384 - sparse_accuracy: 0.7405
test:  f1: 0.81255, precision: 0.81190, recall: 0.81320

Epoch 00009: loss improved from 11.97436 to 11.46990, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 10/300
13/13 [==============================] - 18s 1s/step - loss: 11.1171 - sparse_accuracy: 0.7355
test:  f1: 0.79613, precision: 0.79677, recall: 0.79549

Epoch 00010: loss improved from 11.46990 to 11.09963, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 11/300
13/13 [==============================] - 18s 1s/step - loss: 10.6826 - sparse_accuracy: 0.7443
test:  f1: 0.80735, precision: 0.80159, recall: 0.81320

Epoch 00011: loss improved from 11.09963 to 10.67474, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 12/300
13/13 [==============================] - 18s 1s/step - loss: 10.2735 - sparse_accuracy: 0.7596
test:  f1: 0.81541, precision: 0.81280, recall: 0.81804

Epoch 00012: loss improved from 10.67474 to 10.27695, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 13/300
13/13 [==============================] - 17s 1s/step - loss: 9.8793 - sparse_accuracy: 0.7684
test:  f1: 0.81085, precision: 0.80380, recall: 0.81804

Epoch 00013: loss improved from 10.27695 to 9.87041, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 14/300
13/13 [==============================] - 17s 1s/step - loss: 9.7595 - sparse_accuracy: 0.7662
test:  f1: 0.82035, precision: 0.81005, recall: 0.83092

Epoch 00014: loss improved from 9.87041 to 9.76618, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 15/300
13/13 [==============================] - 18s 1s/step - loss: 9.4708 - sparse_accuracy: 0.7705
test:  f1: 0.82643, precision: 0.81732, recall: 0.83575

Epoch 00015: loss improved from 9.76618 to 9.44313, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 16/300
13/13 [==============================] - 17s 1s/step - loss: 9.3225 - sparse_accuracy: 0.7704
test:  f1: 0.81511, precision: 0.81380, recall: 0.81643

Epoch 00016: loss improved from 9.44313 to 9.32886, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 17/300
13/13 [==============================] - 19s 1s/step - loss: 9.0650 - sparse_accuracy: 0.7709
test:  f1: 0.82390, precision: 0.81546, recall: 0.83253

Epoch 00017: loss improved from 9.32886 to 9.06802, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 18/300
13/13 [==============================] - 17s 1s/step - loss: 8.8595 - sparse_accuracy: 0.7668
test:  f1: 0.82682, precision: 0.81962, recall: 0.83414

Epoch 00018: loss improved from 9.06802 to 8.87142, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 19/300
13/13 [==============================] - 17s 1s/step - loss: 8.6475 - sparse_accuracy: 0.7826
test:  f1: 0.81978, precision: 0.81201, recall: 0.82770

Epoch 00019: loss improved from 8.87142 to 8.63695, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 20/300
13/13 [==============================] - 17s 1s/step - loss: 8.4123 - sparse_accuracy: 0.7792
test:  f1: 0.82737, precision: 0.81761, recall: 0.83736

Epoch 00020: loss improved from 8.63695 to 8.40941, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 21/300
13/13 [==============================] - 18s 1s/step - loss: 8.3810 - sparse_accuracy: 0.7750
test:  f1: 0.82946, precision: 0.82484, recall: 0.83414

Epoch 00021: loss improved from 8.40941 to 8.38482, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 22/300
13/13 [==============================] - 18s 1s/step - loss: 8.2626 - sparse_accuracy: 0.7805
test:  f1: 0.82962, precision: 0.82047, recall: 0.83897

Epoch 00022: loss improved from 8.38482 to 8.25532, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 23/300
13/13 [==============================] - 18s 1s/step - loss: 8.0114 - sparse_accuracy: 0.7837
test:  f1: 0.82109, precision: 0.81458, recall: 0.82770

Epoch 00023: loss improved from 8.25532 to 8.01214, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 24/300
13/13 [==============================] - 18s 1s/step - loss: 7.8900 - sparse_accuracy: 0.7820
test:  f1: 0.81717, precision: 0.80691, recall: 0.82770

Epoch 00024: loss improved from 8.01214 to 7.88795, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 25/300
13/13 [==============================] - 16s 1s/step - loss: 7.7019 - sparse_accuracy: 0.7869
test:  f1: 0.81659, precision: 0.80885, recall: 0.82448

Epoch 00025: loss improved from 7.88795 to 7.70080, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 26/300
13/13 [==============================] - 17s 1s/step - loss: 7.6470 - sparse_accuracy: 0.7793
test:  f1: 0.81529, precision: 0.80630, recall: 0.82448

Epoch 00026: loss improved from 7.70080 to 7.64603, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 27/300
13/13 [==============================] - 18s 1s/step - loss: 7.4637 - sparse_accuracy: 0.7875
test:  f1: 0.81081, precision: 0.80063, recall: 0.82126

Epoch 00027: loss improved from 7.64603 to 7.46489, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 28/300
13/13 [==============================] - 17s 1s/step - loss: 7.2815 - sparse_accuracy: 0.7912
test:  f1: 0.82117, precision: 0.81789, recall: 0.82448

Epoch 00028: loss improved from 7.46489 to 7.28606, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 29/300
13/13 [==============================] - 19s 1s/step - loss: 7.2004 - sparse_accuracy: 0.7941
test:  f1: 0.81059, precision: 0.80800, recall: 0.81320

Epoch 00029: loss improved from 7.28606 to 7.20329, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 30/300
13/13 [==============================] - 17s 1s/step - loss: 7.1209 - sparse_accuracy: 0.7926
test:  f1: 0.80032, precision: 0.79088, recall: 0.80998

Epoch 00030: loss improved from 7.20329 to 7.11276, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 31/300
13/13 [==============================] - 17s 1s/step - loss: 6.9205 - sparse_accuracy: 0.7940
test:  f1: 0.81280, precision: 0.80763, recall: 0.81804

Epoch 00031: loss improved from 7.11276 to 6.93017, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 32/300
13/13 [==============================] - 17s 1s/step - loss: 6.8701 - sparse_accuracy: 0.7969
test:  f1: 0.81340, precision: 0.80569, recall: 0.82126

Epoch 00032: loss improved from 6.93017 to 6.87800, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 33/300
13/13 [==============================] - 17s 1s/step - loss: 6.8155 - sparse_accuracy: 0.7953
test:  f1: 0.81499, precision: 0.80727, recall: 0.82287

Epoch 00033: loss improved from 6.87800 to 6.79140, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 34/300
13/13 [==============================] - 17s 1s/step - loss: 6.6218 - sparse_accuracy: 0.7972
test:  f1: 0.81021, precision: 0.80253, recall: 0.81804

Epoch 00034: loss improved from 6.79140 to 6.62025, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 35/300
13/13 [==============================] - 18s 1s/step - loss: 6.5309 - sparse_accuracy: 0.7932
test:  f1: 0.81275, precision: 0.80442, recall: 0.82126

Epoch 00035: loss improved from 6.62025 to 6.53549, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 36/300
13/13 [==============================] - 17s 1s/step - loss: 6.4766 - sparse_accuracy: 0.7924
test:  f1: 0.80888, precision: 0.79688, recall: 0.82126

Epoch 00036: loss improved from 6.53549 to 6.46847, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 37/300
13/13 [==============================] - 17s 1s/step - loss: 6.3818 - sparse_accuracy: 0.7933
test:  f1: 0.80507, precision: 0.79251, recall: 0.81804

Epoch 00037: loss improved from 6.46847 to 6.38047, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 38/300
13/13 [==============================] - 17s 1s/step - loss: 6.3346 - sparse_accuracy: 0.7985
test:  f1: 0.80096, precision: 0.79213, recall: 0.80998

Epoch 00038: loss improved from 6.38047 to 6.34550, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 39/300
13/13 [==============================] - 18s 1s/step - loss: 6.4263 - sparse_accuracy: 0.7860
test:  f1: 0.81594, precision: 0.80757, recall: 0.82448

Epoch 00039: loss did not improve from 6.34550
Epoch 40/300
13/13 [==============================] - 17s 1s/step - loss: 6.0967 - sparse_accuracy: 0.8054
test:  f1: 0.81285, precision: 0.81090, recall: 0.81481

Epoch 00040: loss improved from 6.34550 to 6.10533, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 41/300
13/13 [==============================] - 17s 1s/step - loss: 6.0851 - sparse_accuracy: 0.8010
test:  f1: 0.81458, precision: 0.80187, recall: 0.82770

Epoch 00041: loss improved from 6.10533 to 6.08011, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 42/300
13/13 [==============================] - 17s 1s/step - loss: 5.9305 - sparse_accuracy: 0.8074
test:  f1: 0.82381, precision: 0.81221, recall: 0.83575

Epoch 00042: loss improved from 6.08011 to 5.92883, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 43/300
13/13 [==============================] - 17s 1s/step - loss: 5.8159 - sparse_accuracy: 0.8076
test:  f1: 0.82194, precision: 0.81162, recall: 0.83253

Epoch 00043: loss improved from 5.92883 to 5.80897, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 44/300
13/13 [==============================] - 17s 1s/step - loss: 5.7678 - sparse_accuracy: 0.8115
test:  f1: 0.81205, precision: 0.80000, recall: 0.82448

Epoch 00044: loss improved from 5.80897 to 5.76330, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 45/300
13/13 [==============================] - 17s 1s/step - loss: 5.7058 - sparse_accuracy: 0.8109
test:  f1: 0.82043, precision: 0.81329, recall: 0.82770

Epoch 00045: loss improved from 5.76330 to 5.70331, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 46/300
13/13 [==============================] - 18s 1s/step - loss: 5.7157 - sparse_accuracy: 0.8081
test:  f1: 0.80929, precision: 0.80542, recall: 0.81320

Epoch 00046: loss did not improve from 5.70331
Epoch 47/300
13/13 [==============================] - 18s 1s/step - loss: 5.5555 - sparse_accuracy: 0.8133
test:  f1: 0.83028, precision: 0.82177, recall: 0.83897

Epoch 00047: loss improved from 5.70331 to 5.54890, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 48/300
13/13 [==============================] - 18s 1s/step - loss: 5.4510 - sparse_accuracy: 0.8195
test:  f1: 0.79937, precision: 0.78750, recall: 0.81159

Epoch 00048: loss improved from 5.54890 to 5.45841, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 49/300
13/13 [==============================] - 18s 1s/step - loss: 5.3351 - sparse_accuracy: 0.8267
test:  f1: 0.81717, precision: 0.80691, recall: 0.82770

Epoch 00049: loss improved from 5.45841 to 5.32674, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 50/300
13/13 [==============================] - 18s 1s/step - loss: 5.2565 - sparse_accuracy: 0.8305
test:  f1: 0.81201, precision: 0.79690, recall: 0.82770

Epoch 00050: loss improved from 5.32674 to 5.24895, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 51/300
13/13 [==============================] - 18s 1s/step - loss: 5.1452 - sparse_accuracy: 0.8351
test:  f1: 0.79485, precision: 0.79421, recall: 0.79549

Epoch 00051: loss improved from 5.24895 to 5.14049, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 52/300
13/13 [==============================] - 16s 1s/step - loss: 5.1528 - sparse_accuracy: 0.8257
test:  f1: 0.82157, precision: 0.80938, recall: 0.83414

Epoch 00052: loss did not improve from 5.14049
Epoch 53/300
13/13 [==============================] - 18s 1s/step - loss: 5.0310 - sparse_accuracy: 0.8300
test:  f1: 0.82819, precision: 0.81464, recall: 0.84219

Epoch 00053: loss improved from 5.14049 to 5.04189, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 54/300
13/13 [==============================] - 17s 1s/step - loss: 5.0862 - sparse_accuracy: 0.8299
test:  f1: 0.81394, precision: 0.80062, recall: 0.82770

Epoch 00054: loss did not improve from 5.04189
Epoch 55/300
13/13 [==============================] - 18s 1s/step - loss: 5.0450 - sparse_accuracy: 0.8347
test:  f1: 0.80472, precision: 0.78737, recall: 0.82287

Epoch 00055: loss did not improve from 5.04189
Epoch 56/300
13/13 [==============================] - 18s 1s/step - loss: 4.9025 - sparse_accuracy: 0.8425
test:  f1: 0.80851, precision: 0.79167, recall: 0.82609

Epoch 00056: loss improved from 5.04189 to 4.90378, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 57/300
13/13 [==============================] - 17s 1s/step - loss: 4.7780 - sparse_accuracy: 0.8443
test:  f1: 0.80223, precision: 0.79463, recall: 0.80998

Epoch 00057: loss improved from 4.90378 to 4.77361, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 58/300
13/13 [==============================] - 17s 1s/step - loss: 4.7481 - sparse_accuracy: 0.8431
test:  f1: 0.80317, precision: 0.79186, recall: 0.81481

Epoch 00058: loss improved from 4.77361 to 4.74543, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 59/300
13/13 [==============================] - 17s 1s/step - loss: 4.5598 - sparse_accuracy: 0.8493
test:  f1: 0.81552, precision: 0.80218, recall: 0.82931

Epoch 00059: loss improved from 4.74543 to 4.56414, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 60/300
13/13 [==============================] - 18s 1s/step - loss: 4.6494 - sparse_accuracy: 0.8414
test:  f1: 0.80885, precision: 0.79380, recall: 0.82448

Epoch 00060: loss did not improve from 4.56414
Epoch 61/300
13/13 [==============================] - 18s 1s/step - loss: 4.5763 - sparse_accuracy: 0.8456
test:  f1: 0.80699, precision: 0.79624, recall: 0.81804

Epoch 00061: loss did not improve from 4.56414
Epoch 62/300
13/13 [==============================] - 17s 1s/step - loss: 4.4135 - sparse_accuracy: 0.8537
test:  f1: 0.81558, precision: 0.80534, recall: 0.82609

Epoch 00062: loss improved from 4.56414 to 4.40603, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 63/300
13/13 [==============================] - 18s 1s/step - loss: 4.4539 - sparse_accuracy: 0.8456
test:  f1: 0.81623, precision: 0.80660, recall: 0.82609

Epoch 00063: loss did not improve from 4.40603
Epoch 64/300
13/13 [==============================] - 18s 1s/step - loss: 4.4301 - sparse_accuracy: 0.8486
test:  f1: 0.82504, precision: 0.82240, recall: 0.82770

Epoch 00064: loss did not improve from 4.40603
Epoch 65/300
13/13 [==============================] - 17s 1s/step - loss: 4.3222 - sparse_accuracy: 0.8450
test:  f1: 0.81804, precision: 0.80404, recall: 0.83253

Epoch 00065: loss improved from 4.40603 to 4.30562, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 66/300
13/13 [==============================] - 17s 1s/step - loss: 4.2998 - sparse_accuracy: 0.8498
test:  f1: 0.81464, precision: 0.80503, recall: 0.82448

Epoch 00066: loss improved from 4.30562 to 4.28204, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 67/300
13/13 [==============================] - 18s 1s/step - loss: 4.3986 - sparse_accuracy: 0.8465
test:  f1: 0.79650, precision: 0.78650, recall: 0.80676

Epoch 00067: loss did not improve from 4.28204
Epoch 68/300
13/13 [==============================] - 18s 1s/step - loss: 4.1758 - sparse_accuracy: 0.8522
test:  f1: 0.82446, precision: 0.81348, recall: 0.83575

Epoch 00068: loss improved from 4.28204 to 4.17612, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 69/300
13/13 [==============================] - 17s 1s/step - loss: 4.0690 - sparse_accuracy: 0.8587
test:  f1: 0.80983, precision: 0.79719, recall: 0.82287

Epoch 00069: loss improved from 4.17612 to 4.07376, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 70/300
13/13 [==============================] - 18s 1s/step - loss: 4.0348 - sparse_accuracy: 0.8614
test:  f1: 0.80159, precision: 0.79030, recall: 0.81320

Epoch 00070: loss improved from 4.07376 to 4.03623, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 71/300
13/13 [==============================] - 18s 1s/step - loss: 3.9121 - sparse_accuracy: 0.8618
test:  f1: 0.80730, precision: 0.79531, recall: 0.81965

Epoch 00071: loss improved from 4.03623 to 3.91899, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 72/300
13/13 [==============================] - 18s 1s/step - loss: 3.8309 - sparse_accuracy: 0.8615
test:  f1: 0.80191, precision: 0.79245, recall: 0.81159

Epoch 00072: loss improved from 3.91899 to 3.83816, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 73/300
13/13 [==============================] - 17s 1s/step - loss: 3.8128 - sparse_accuracy: 0.8640
test:  f1: 0.80507, precision: 0.79251, recall: 0.81804

Epoch 00073: loss improved from 3.83816 to 3.81095, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 74/300
13/13 [==============================] - 18s 1s/step - loss: 3.7365 - sparse_accuracy: 0.8642
test:  f1: 0.81364, precision: 0.80156, recall: 0.82609

Epoch 00074: loss improved from 3.81095 to 3.72478, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 75/300
13/13 [==============================] - 17s 1s/step - loss: 3.6736 - sparse_accuracy: 0.8633
test:  f1: 0.81201, precision: 0.79690, recall: 0.82770

Epoch 00075: loss improved from 3.72478 to 3.67041, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 76/300
13/13 [==============================] - 17s 1s/step - loss: 3.6632 - sparse_accuracy: 0.8666
test:  f1: 0.80760, precision: 0.79439, recall: 0.82126

Epoch 00076: loss improved from 3.67041 to 3.66625, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 77/300
13/13 [==============================] - 18s 1s/step - loss: 3.6381 - sparse_accuracy: 0.8596
test:  f1: 0.81334, precision: 0.80251, recall: 0.82448

Epoch 00077: loss improved from 3.66625 to 3.63663, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 78/300
13/13 [==============================] - 18s 1s/step - loss: 3.5447 - sparse_accuracy: 0.8676
test:  f1: 0.80000, precision: 0.78873, recall: 0.81159

Epoch 00078: loss improved from 3.63663 to 3.55255, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 79/300
13/13 [==============================] - 17s 1s/step - loss: 3.4995 - sparse_accuracy: 0.8662
test:  f1: 0.80542, precision: 0.79779, recall: 0.81320

Epoch 00079: loss improved from 3.55255 to 3.49563, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 80/300
13/13 [==============================] - 17s 1s/step - loss: 3.4481 - sparse_accuracy: 0.8683
test:  f1: 0.80827, precision: 0.79874, recall: 0.81804

Epoch 00080: loss improved from 3.49563 to 3.44658, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 81/300
13/13 [==============================] - 17s 1s/step - loss: 3.3767 - sparse_accuracy: 0.8703
test:  f1: 0.80791, precision: 0.79348, recall: 0.82287

Epoch 00081: loss improved from 3.44658 to 3.37418, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 82/300
13/13 [==============================] - 18s 1s/step - loss: 3.3943 - sparse_accuracy: 0.8702
test:  f1: 0.81210, precision: 0.80315, recall: 0.82126

Epoch 00082: loss did not improve from 3.37418
Epoch 83/300
13/13 [==============================] - 17s 1s/step - loss: 3.3429 - sparse_accuracy: 0.8703
test:  f1: 0.81235, precision: 0.79907, recall: 0.82609

Epoch 00083: loss improved from 3.37418 to 3.34072, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 84/300
13/13 [==============================] - 18s 1s/step - loss: 3.2873 - sparse_accuracy: 0.8694
test:  f1: 0.81717, precision: 0.80691, recall: 0.82770

Epoch 00084: loss improved from 3.34072 to 3.29115, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 85/300
13/13 [==============================] - 18s 1s/step - loss: 3.2447 - sparse_accuracy: 0.8694
test:  f1: 0.80983, precision: 0.79719, recall: 0.82287

Epoch 00085: loss improved from 3.29115 to 3.24516, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 86/300
13/13 [==============================] - 17s 1s/step - loss: 3.2587 - sparse_accuracy: 0.8715
test:  f1: 0.81205, precision: 0.80000, recall: 0.82448

Epoch 00086: loss did not improve from 3.24516
Epoch 87/300
13/13 [==============================] - 18s 1s/step - loss: 3.1415 - sparse_accuracy: 0.8691
test:  f1: 0.81717, precision: 0.80691, recall: 0.82770

Epoch 00087: loss improved from 3.24516 to 3.13995, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 88/300
13/13 [==============================] - 18s 1s/step - loss: 3.0871 - sparse_accuracy: 0.8682
test:  f1: 0.81552, precision: 0.80218, recall: 0.82931

Epoch 00088: loss improved from 3.13995 to 3.09335, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 89/300
13/13 [==============================] - 17s 1s/step - loss: 3.1015 - sparse_accuracy: 0.8719
test:  f1: 0.81077, precision: 0.79751, recall: 0.82448

Epoch 00089: loss did not improve from 3.09335
Epoch 90/300
13/13 [==============================] - 17s 1s/step - loss: 3.0504 - sparse_accuracy: 0.8736
test:  f1: 0.80824, precision: 0.79563, recall: 0.82126

Epoch 00090: loss improved from 3.09335 to 3.05180, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 91/300
13/13 [==============================] - 17s 1s/step - loss: 3.0403 - sparse_accuracy: 0.8713
test:  f1: 0.81646, precision: 0.80249, recall: 0.83092

Epoch 00091: loss improved from 3.05180 to 3.04566, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 92/300
13/13 [==============================] - 17s 1s/step - loss: 2.9971 - sparse_accuracy: 0.8745
test:  f1: 0.82047, precision: 0.80277, recall: 0.83897

Epoch 00092: loss improved from 3.04566 to 3.00242, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 93/300
13/13 [==============================] - 18s 1s/step - loss: 2.9171 - sparse_accuracy: 0.8726
test:  f1: 0.80979, precision: 0.79412, recall: 0.82609

Epoch 00093: loss improved from 3.00242 to 2.91732, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 94/300
13/13 [==============================] - 17s 1s/step - loss: 2.8514 - sparse_accuracy: 0.8793
test:  f1: 0.81616, precision: 0.80343, recall: 0.82931

Epoch 00094: loss improved from 2.91732 to 2.84708, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 95/300
13/13 [==============================] - 17s 1s/step - loss: 2.8380 - sparse_accuracy: 0.8779
test:  f1: 0.81452, precision: 0.79876, recall: 0.83092

Epoch 00095: loss improved from 2.84708 to 2.83400, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 96/300
13/13 [==============================] - 18s 1s/step - loss: 2.8382 - sparse_accuracy: 0.8752
test:  f1: 0.81359, precision: 0.79845, recall: 0.82931

Epoch 00096: loss did not improve from 2.83400
Epoch 97/300
13/13 [==============================] - 17s 1s/step - loss: 2.8188 - sparse_accuracy: 0.8771
test:  f1: 0.81610, precision: 0.80031, recall: 0.83253

Epoch 00097: loss improved from 2.83400 to 2.82289, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 98/300
13/13 [==============================] - 18s 1s/step - loss: 2.7840 - sparse_accuracy: 0.8729
test:  f1: 0.80505, precision: 0.78947, recall: 0.82126

Epoch 00098: loss improved from 2.82289 to 2.78770, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 99/300
13/13 [==============================] - 18s 1s/step - loss: 2.7087 - sparse_accuracy: 0.8794
test:  f1: 0.80760, precision: 0.79439, recall: 0.82126

Epoch 00099: loss improved from 2.78770 to 2.71521, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 100/300
13/13 [==============================] - 11s 852ms/step - loss: 2.7042 - sparse_accuracy: 0.8825
test:  f1: 0.81423, precision: 0.79969, recall: 0.82931

Epoch 00100: loss improved from 2.71521 to 2.70064, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 101/300
13/13 [==============================] - 16s 1s/step - loss: 2.6637 - sparse_accuracy: 0.8775
test:  f1: 0.81043, precision: 0.79535, recall: 0.82609

Epoch 00101: loss improved from 2.70064 to 2.66752, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 102/300
13/13 [==============================] - 17s 1s/step - loss: 2.6777 - sparse_accuracy: 0.8726
test:  f1: 0.81077, precision: 0.79751, recall: 0.82448

Epoch 00102: loss did not improve from 2.66752
Epoch 103/300
13/13 [==============================] - 18s 1s/step - loss: 2.6414 - sparse_accuracy: 0.8748
test:  f1: 0.80787, precision: 0.79045, recall: 0.82609

Epoch 00103: loss improved from 2.66752 to 2.64519, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 104/300
13/13 [==============================] - 16s 1s/step - loss: 2.6379 - sparse_accuracy: 0.8735
test:  f1: 0.81552, precision: 0.80218, recall: 0.82931

Epoch 00104: loss improved from 2.64519 to 2.62695, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 105/300
13/13 [==============================] - 17s 1s/step - loss: 2.6318 - sparse_accuracy: 0.8733
test:  f1: 0.81487, precision: 0.80093, recall: 0.82931

Epoch 00105: loss did not improve from 2.62695
Epoch 106/300
13/13 [==============================] - 18s 1s/step - loss: 2.5795 - sparse_accuracy: 0.8740
test:  f1: 0.81294, precision: 0.79721, recall: 0.82931

Epoch 00106: loss improved from 2.62695 to 2.57985, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 107/300
13/13 [==============================] - 18s 1s/step - loss: 2.5446 - sparse_accuracy: 0.8790
test:  f1: 0.80824, precision: 0.79563, recall: 0.82126

Epoch 00107: loss improved from 2.57985 to 2.53781, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 108/300
13/13 [==============================] - 17s 1s/step - loss: 2.5212 - sparse_accuracy: 0.8752
test:  f1: 0.81171, precision: 0.79782, recall: 0.82609

Epoch 00108: loss improved from 2.53781 to 2.52398, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 109/300
13/13 [==============================] - 19s 1s/step - loss: 2.4576 - sparse_accuracy: 0.8813
test:  f1: 0.81141, precision: 0.79875, recall: 0.82448

Epoch 00109: loss improved from 2.52398 to 2.45659, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 110/300
13/13 [==============================] - 17s 1s/step - loss: 2.4584 - sparse_accuracy: 0.8751
test:  f1: 0.80791, precision: 0.79348, recall: 0.82287

Epoch 00110: loss improved from 2.45659 to 2.45358, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 111/300
13/13 [==============================] - 17s 1s/step - loss: 2.4223 - sparse_accuracy: 0.8786
test:  f1: 0.81334, precision: 0.80251, recall: 0.82448

Epoch 00111: loss improved from 2.45358 to 2.41884, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 112/300
13/13 [==============================] - 17s 1s/step - loss: 2.4436 - sparse_accuracy: 0.8818
test:  f1: 0.81558, precision: 0.80534, recall: 0.82609

Epoch 00112: loss did not improve from 2.41884
Epoch 113/300
13/13 [==============================] - 18s 1s/step - loss: 2.5017 - sparse_accuracy: 0.8709
test:  f1: 0.80760, precision: 0.79439, recall: 0.82126

Epoch 00113: loss did not improve from 2.41884
Epoch 114/300
13/13 [==============================] - 18s 1s/step - loss: 2.4576 - sparse_accuracy: 0.8720
test:  f1: 0.80032, precision: 0.79398, recall: 0.80676

Epoch 00114: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.

Epoch 00114: loss did not improve from 2.41884
Epoch 115/300
13/13 [==============================] - 20s 2s/step - loss: 2.4508 - sparse_accuracy: 0.8727
test:  f1: 0.81359, precision: 0.79845, recall: 0.82931

Epoch 00115: loss did not improve from 2.41884
Epoch 116/300
13/13 [==============================] - 18s 1s/step - loss: 2.3104 - sparse_accuracy: 0.8806
test:  f1: 0.80851, precision: 0.79167, recall: 0.82609

Epoch 00116: loss improved from 2.41884 to 2.31526, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 117/300
13/13 [==============================] - 17s 1s/step - loss: 2.3021 - sparse_accuracy: 0.8762
test:  f1: 0.81840, precision: 0.80625, recall: 0.83092

Epoch 00117: loss improved from 2.31526 to 2.30446, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 118/300
13/13 [==============================] - 17s 1s/step - loss: 2.2534 - sparse_accuracy: 0.8758
test:  f1: 0.80949, precision: 0.79503, recall: 0.82448

Epoch 00118: loss improved from 2.30446 to 2.25728, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 119/300
13/13 [==============================] - 17s 1s/step - loss: 2.2777 - sparse_accuracy: 0.8770
test:  f1: 0.81230, precision: 0.79598, recall: 0.82931

Epoch 00119: loss did not improve from 2.25728
Epoch 120/300
13/13 [==============================] - 17s 1s/step - loss: 2.2573 - sparse_accuracy: 0.8786
test:  f1: 0.81359, precision: 0.79845, recall: 0.82931

Epoch 00120: loss did not improve from 2.25728
Epoch 121/300
13/13 [==============================] - 17s 1s/step - loss: 2.2660 - sparse_accuracy: 0.8749
test:  f1: 0.81359, precision: 0.79845, recall: 0.82931

Epoch 00121: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.

Epoch 00121: loss did not improve from 2.25728
Epoch 122/300
13/13 [==============================] - 18s 1s/step - loss: 2.2261 - sparse_accuracy: 0.8789
test:  f1: 0.81359, precision: 0.79845, recall: 0.82931

Epoch 00122: loss improved from 2.25728 to 2.22252, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 123/300
13/13 [==============================] - 18s 1s/step - loss: 2.2251 - sparse_accuracy: 0.8776
test:  f1: 0.81359, precision: 0.79845, recall: 0.82931

Epoch 00123: loss improved from 2.22252 to 2.21965, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 124/300
13/13 [==============================] - 17s 1s/step - loss: 2.2042 - sparse_accuracy: 0.8797
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00124: loss improved from 2.21965 to 2.20720, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 125/300
13/13 [==============================] - 18s 1s/step - loss: 2.2176 - sparse_accuracy: 0.8837
test:  f1: 0.81329, precision: 0.79938, recall: 0.82770

Epoch 00125: loss did not improve from 2.20720
Epoch 126/300
13/13 [==============================] - 18s 1s/step - loss: 2.2189 - sparse_accuracy: 0.8791
test:  f1: 0.81423, precision: 0.79969, recall: 0.82931

Epoch 00126: loss did not improve from 2.20720
Epoch 127/300
13/13 [==============================] - 18s 1s/step - loss: 2.1486 - sparse_accuracy: 0.8772
test:  f1: 0.81294, precision: 0.79721, recall: 0.82931

Epoch 00127: loss improved from 2.20720 to 2.15327, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 128/300
13/13 [==============================] - 17s 1s/step - loss: 2.1975 - sparse_accuracy: 0.8782
test:  f1: 0.81294, precision: 0.79721, recall: 0.82931

Epoch 00128: loss did not improve from 2.15327
Epoch 129/300
13/13 [==============================] - 18s 1s/step - loss: 2.1551 - sparse_accuracy: 0.8800
test:  f1: 0.81107, precision: 0.79658, recall: 0.82609

Epoch 00129: loss did not improve from 2.15327
Epoch 130/300
13/13 [==============================] - 16s 1s/step - loss: 2.1723 - sparse_accuracy: 0.8771
test:  f1: 0.81487, precision: 0.80093, recall: 0.82931

Epoch 00130: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.

Epoch 00130: loss did not improve from 2.15327
Epoch 131/300
13/13 [==============================] - 17s 1s/step - loss: 2.1890 - sparse_accuracy: 0.8801
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00131: loss did not improve from 2.15327
Epoch 132/300
13/13 [==============================] - 18s 1s/step - loss: 2.1905 - sparse_accuracy: 0.8784
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00132: loss did not improve from 2.15327
Epoch 133/300
13/13 [==============================] - 17s 1s/step - loss: 2.1600 - sparse_accuracy: 0.8784
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00133: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.

Epoch 00133: loss did not improve from 2.15327
Epoch 134/300
13/13 [==============================] - 17s 1s/step - loss: 2.1412 - sparse_accuracy: 0.8801
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00134: loss improved from 2.15327 to 2.14468, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 135/300
13/13 [==============================] - 17s 1s/step - loss: 2.2229 - sparse_accuracy: 0.8756
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00135: loss did not improve from 2.14468
Epoch 136/300
13/13 [==============================] - 16s 1s/step - loss: 2.1568 - sparse_accuracy: 0.8827
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00136: loss did not improve from 2.14468
Epoch 137/300
13/13 [==============================] - 17s 1s/step - loss: 2.1351 - sparse_accuracy: 0.8819
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00137: loss improved from 2.14468 to 2.13077, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 138/300
13/13 [==============================] - 17s 1s/step - loss: 2.1290 - sparse_accuracy: 0.8785
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00138: loss did not improve from 2.13077
Epoch 139/300
13/13 [==============================] - 16s 1s/step - loss: 2.1644 - sparse_accuracy: 0.8814
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00139: loss did not improve from 2.13077
Epoch 140/300
13/13 [==============================] - 17s 1s/step - loss: 2.1970 - sparse_accuracy: 0.8770
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00140: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.

Epoch 00140: loss did not improve from 2.13077
Epoch 141/300
13/13 [==============================] - 18s 1s/step - loss: 2.1591 - sparse_accuracy: 0.8769
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00141: loss did not improve from 2.13077
Epoch 142/300
13/13 [==============================] - 17s 1s/step - loss: 2.1103 - sparse_accuracy: 0.8777
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00142: loss improved from 2.13077 to 2.11544, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 143/300
13/13 [==============================] - 18s 1s/step - loss: 2.1124 - sparse_accuracy: 0.8805
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00143: loss improved from 2.11544 to 2.10325, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 144/300
13/13 [==============================] - 18s 1s/step - loss: 2.1990 - sparse_accuracy: 0.8765
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00144: loss did not improve from 2.10325
Epoch 145/300
13/13 [==============================] - 18s 1s/step - loss: 2.1277 - sparse_accuracy: 0.8820
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00145: loss did not improve from 2.10325
Epoch 146/300
13/13 [==============================] - 17s 1s/step - loss: 2.1565 - sparse_accuracy: 0.8750
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00146: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.

Epoch 00146: loss did not improve from 2.10325
Epoch 147/300
13/13 [==============================] - 19s 1s/step - loss: 2.1120 - sparse_accuracy: 0.8791
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00147: loss did not improve from 2.10325
Epoch 148/300
13/13 [==============================] - 18s 1s/step - loss: 2.1497 - sparse_accuracy: 0.8792
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00148: loss did not improve from 2.10325
Epoch 149/300
13/13 [==============================] - 17s 1s/step - loss: 2.1248 - sparse_accuracy: 0.8803
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00149: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.

Epoch 00149: loss did not improve from 2.10325
Epoch 150/300
13/13 [==============================] - 18s 1s/step - loss: 2.1032 - sparse_accuracy: 0.8840
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00150: loss improved from 2.10325 to 2.10165, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_3.h5
Epoch 151/300
13/13 [==============================] - 18s 1s/step - loss: 2.1545 - sparse_accuracy: 0.8784
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00151: loss did not improve from 2.10165
Epoch 152/300
13/13 [==============================] - 18s 1s/step - loss: 2.1225 - sparse_accuracy: 0.8779
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00152: loss did not improve from 2.10165
Epoch 153/300
13/13 [==============================] - 16s 1s/step - loss: 2.1736 - sparse_accuracy: 0.8793
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00153: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.

Epoch 00153: loss did not improve from 2.10165
Epoch 154/300
13/13 [==============================] - 18s 1s/step - loss: 2.1317 - sparse_accuracy: 0.8788
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00154: loss did not improve from 2.10165
Epoch 155/300
13/13 [==============================] - 17s 1s/step - loss: 2.1390 - sparse_accuracy: 0.8774
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00155: loss did not improve from 2.10165
Epoch 156/300
13/13 [==============================] - 18s 1s/step - loss: 2.1541 - sparse_accuracy: 0.8744
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00156: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.

Epoch 00156: loss did not improve from 2.10165
Epoch 157/300
13/13 [==============================] - 17s 1s/step - loss: 2.1078 - sparse_accuracy: 0.8821
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00157: loss did not improve from 2.10165
Epoch 158/300
13/13 [==============================] - 18s 1s/step - loss: 2.1570 - sparse_accuracy: 0.8769
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00158: loss did not improve from 2.10165
Epoch 159/300
13/13 [==============================] - 18s 1s/step - loss: 2.1297 - sparse_accuracy: 0.8790
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00159: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.

Epoch 00159: loss did not improve from 2.10165
Epoch 160/300
13/13 [==============================] - 18s 1s/step - loss: 2.1627 - sparse_accuracy: 0.8744
test:  f1: 0.81739, precision: 0.80280, recall: 0.83253

Epoch 00160: loss did not improve from 2.10165
Epoch 00160: early stopping

进程已结束,退出代码0
