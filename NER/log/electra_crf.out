ssh://bureaux@172.30.2.148:22/home/bureaux/miniconda3/envs/Keras-base/bin/python -u /home/bureaux/Projects/keras4bert/NER/train.py
Using TensorFlow backend.
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.

WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.

__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
Input-Token (InputLayer)        (None, None)         0
__________________________________________________________________________________________________
Input-Segment (InputLayer)      (None, None)         0
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 768)    16226304    Input-Token[0][0]
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]
                                                                 Embedding-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Position[0][0]
__________________________________________________________________________________________________
Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]
                                                                 Embedding-Dropout[0][0]
                                                                 Embedding-Dropout[0][0]
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]
                                                                 Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward (Feed (None, None, 768)    4722432     Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward-Dropo (None, None, 768)    0           Transformer-0-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-0-FeedForward-Add ( (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
                                                                 Transformer-0-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-0-FeedForward-Norm  (None, None, 768)    1536        Transformer-0-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 768)    4722432     Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 768)    1536        Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward (Feed (None, None, 768)    4722432     Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
                                                                 Transformer-4-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-4-FeedForward-Norm  (None, None, 768)    1536        Transformer-4-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward (Feed (None, None, 768)    4722432     Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
                                                                 Transformer-5-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-5-FeedForward-Norm  (None, None, 768)    1536        Transformer-5-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward (Feed (None, None, 768)    4722432     Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward-Dropo (None, None, 768)    0           Transformer-6-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-6-FeedForward-Add ( (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
                                                                 Transformer-6-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-6-FeedForward-Norm  (None, None, 768)    1536        Transformer-6-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward (Feed (None, None, 768)    4722432     Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward-Dropo (None, None, 768)    0           Transformer-7-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-7-FeedForward-Add ( (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
                                                                 Transformer-7-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-7-FeedForward-Norm  (None, None, 768)    1536        Transformer-7-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward (Feed (None, None, 768)    4722432     Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward-Dropo (None, None, 768)    0           Transformer-8-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-8-FeedForward-Add ( (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
                                                                 Transformer-8-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-8-FeedForward-Norm  (None, None, 768)    1536        Transformer-8-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward (Feed (None, None, 768)    4722432     Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward-Dropo (None, None, 768)    0           Transformer-9-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-9-FeedForward-Add ( (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
                                                                 Transformer-9-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-9-FeedForward-Norm  (None, None, 768)    1536        Transformer-9-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward (Fee (None, None, 768)    4722432     Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward-Drop (None, None, 768)    0           Transformer-10-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-10-FeedForward-Add  (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
                                                                 Transformer-10-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-10-FeedForward-Norm (None, None, 768)    1536        Transformer-10-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-FeedForward-Norm[0
                                                                 Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward (Fee (None, None, 768)    4722432     Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward-Drop (None, None, 768)    0           Transformer-11-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-11-FeedForward-Add  (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
                                                                 Transformer-11-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-11-FeedForward-Norm (None, None, 768)    1536        Transformer-11-FeedForward-Add[0]
__________________________________________________________________________________________________
dense_73 (Dense)                (None, None, 29)     22301       Transformer-11-FeedForward-Norm[0
__________________________________________________________________________________________________
conditional_random_field_1 (Con (None, None, 29)     841         dense_73[0][0]
==================================================================================================
Total params: 101,700,198
Trainable params: 101,700,198
Non-trainable params: 0
__________________________________________________________________________________________________
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:2403: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Epoch 1/300
13/13 [==============================] - 24s 2s/step - loss: 35.8371 - sparse_accuracy: 0.6013
test:  f1: 0.71192, precision: 0.69814, recall: 0.72625

Epoch 00001: loss improved from inf to 36.42207, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 2/300
13/13 [==============================] - 9s 686ms/step - loss: 8.1272 - sparse_accuracy: 0.8320
test:  f1: 0.73466, precision: 0.72713, recall: 0.74235

Epoch 00002: loss improved from 36.42207 to 8.15688, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 3/300
13/13 [==============================] - 9s 654ms/step - loss: 4.3957 - sparse_accuracy: 0.8678
test:  f1: 0.79779, precision: 0.78295, recall: 0.81320

Epoch 00003: loss improved from 8.15688 to 4.40027, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 4/300
13/13 [==============================] - 9s 672ms/step - loss: 2.9271 - sparse_accuracy: 0.8848
test:  f1: 0.82605, precision: 0.81505, recall: 0.83736

Epoch 00004: loss improved from 4.40027 to 2.95128, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 5/300
13/13 [==============================] - 9s 715ms/step - loss: 2.1108 - sparse_accuracy: 0.8960
test:  f1: 0.82605, precision: 0.81505, recall: 0.83736

Epoch 00005: loss improved from 2.95128 to 2.10800, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 6/300
13/13 [==============================] - 9s 718ms/step - loss: 1.6502 - sparse_accuracy: 0.9121
test:  f1: 0.83439, precision: 0.82520, recall: 0.84380

Epoch 00006: loss improved from 2.10800 to 1.65155, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 7/300
13/13 [==============================] - 8s 635ms/step - loss: 1.3263 - sparse_accuracy: 0.9137
test:  f1: 0.81796, precision: 0.81470, recall: 0.82126

Epoch 00007: loss improved from 1.65155 to 1.31534, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 8/300
13/13 [==============================] - 9s 663ms/step - loss: 1.0314 - sparse_accuracy: 0.9294
test:  f1: 0.81587, precision: 0.80438, recall: 0.82770

Epoch 00008: loss improved from 1.31534 to 1.01974, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 9/300
13/13 [==============================] - 9s 680ms/step - loss: 0.9338 - sparse_accuracy: 0.9301
test:  f1: 0.81171, precision: 0.79782, recall: 0.82609

Epoch 00009: loss improved from 1.01974 to 0.93290, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 10/300
13/13 [==============================] - 9s 703ms/step - loss: 0.7834 - sparse_accuracy: 0.9360
test:  f1: 0.80990, precision: 0.80349, recall: 0.81643

Epoch 00010: loss improved from 0.93290 to 0.77782, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 11/300
13/13 [==============================] - 9s 711ms/step - loss: 0.6627 - sparse_accuracy: 0.9436
test:  f1: 0.82588, precision: 0.81933, recall: 0.83253

Epoch 00011: loss improved from 0.77782 to 0.65166, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 12/300
13/13 [==============================] - 9s 671ms/step - loss: 0.6365 - sparse_accuracy: 0.9431
test:  f1: 0.80831, precision: 0.80190, recall: 0.81481

Epoch 00012: loss improved from 0.65166 to 0.62339, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 13/300
13/13 [==============================] - 9s 671ms/step - loss: 0.5577 - sparse_accuracy: 0.9512
test:  f1: 0.81665, precision: 0.81210, recall: 0.82126

Epoch 00013: loss improved from 0.62339 to 0.56047, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 14/300
13/13 [==============================] - 9s 676ms/step - loss: 0.5568 - sparse_accuracy: 0.9493
test:  f1: 0.81529, precision: 0.80630, recall: 0.82448

Epoch 00014: loss improved from 0.56047 to 0.55648, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 15/300
13/13 [==============================] - 9s 680ms/step - loss: 0.4732 - sparse_accuracy: 0.9519
test:  f1: 0.82297, precision: 0.81517, recall: 0.83092

Epoch 00015: loss improved from 0.55648 to 0.47903, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 16/300
13/13 [==============================] - 9s 690ms/step - loss: 0.4291 - sparse_accuracy: 0.9585
test:  f1: 0.82946, precision: 0.82484, recall: 0.83414

Epoch 00016: loss improved from 0.47903 to 0.42828, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 17/300
13/13 [==============================] - 9s 689ms/step - loss: 0.3856 - sparse_accuracy: 0.9607
test:  f1: 0.81746, precision: 0.80595, recall: 0.82931

Epoch 00017: loss improved from 0.42828 to 0.38801, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 18/300
13/13 [==============================] - 9s 663ms/step - loss: 0.4077 - sparse_accuracy: 0.9567
test:  f1: 0.82919, precision: 0.82588, recall: 0.83253

Epoch 00018: loss did not improve from 0.38801
Epoch 19/300
13/13 [==============================] - 9s 700ms/step - loss: 0.3840 - sparse_accuracy: 0.9564
test:  f1: 0.81811, precision: 0.80721, recall: 0.82931

Epoch 00019: loss did not improve from 0.38801
Epoch 20/300
13/13 [==============================] - 9s 686ms/step - loss: 0.4061 - sparse_accuracy: 0.9544
test:  f1: 0.82919, precision: 0.82588, recall: 0.83253

Epoch 00020: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.

Epoch 00020: loss did not improve from 0.38801
Epoch 21/300
13/13 [==============================] - 9s 691ms/step - loss: 0.2947 - sparse_accuracy: 0.9630
test:  f1: 0.82100, precision: 0.81132, recall: 0.83092

Epoch 00021: loss improved from 0.38801 to 0.30020, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 22/300
13/13 [==============================] - 9s 718ms/step - loss: 0.2401 - sparse_accuracy: 0.9660
test:  f1: 0.83160, precision: 0.82437, recall: 0.83897

Epoch 00022: loss improved from 0.30020 to 0.24212, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 23/300
13/13 [==============================] - 10s 746ms/step - loss: 0.2651 - sparse_accuracy: 0.9664
test:  f1: 0.82297, precision: 0.81517, recall: 0.83092

Epoch 00023: loss did not improve from 0.24212
Epoch 24/300
13/13 [==============================] - 10s 771ms/step - loss: 0.2240 - sparse_accuracy: 0.9695
test:  f1: 0.82072, precision: 0.81230, recall: 0.82931

Epoch 00024: loss improved from 0.24212 to 0.22446, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 25/300
13/13 [==============================] - 9s 699ms/step - loss: 0.2312 - sparse_accuracy: 0.9696
test:  f1: 0.81876, precision: 0.80848, recall: 0.82931

Epoch 00025: loss did not improve from 0.22446
Epoch 26/300
13/13 [==============================] - 10s 804ms/step - loss: 0.2282 - sparse_accuracy: 0.9691
test:  f1: 0.83094, precision: 0.82306, recall: 0.83897

Epoch 00026: loss did not improve from 0.22446
Epoch 27/300
13/13 [==============================] - 10s 741ms/step - loss: 0.2122 - sparse_accuracy: 0.9711
test:  f1: 0.81847, precision: 0.80945, recall: 0.82770

Epoch 00027: loss improved from 0.22446 to 0.20398, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 28/300
13/13 [==============================] - 9s 724ms/step - loss: 0.2030 - sparse_accuracy: 0.9727
test:  f1: 0.81847, precision: 0.80945, recall: 0.82770

Epoch 00028: loss improved from 0.20398 to 0.20327, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 29/300
13/13 [==============================] - 9s 677ms/step - loss: 0.1878 - sparse_accuracy: 0.9710
test:  f1: 0.81978, precision: 0.81201, recall: 0.82770

Epoch 00029: loss improved from 0.20327 to 0.19021, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 30/300
13/13 [==============================] - 9s 723ms/step - loss: 0.1950 - sparse_accuracy: 0.9729
test:  f1: 0.82035, precision: 0.81005, recall: 0.83092

Epoch 00030: loss did not improve from 0.19021
Epoch 31/300
13/13 [==============================] - 9s 728ms/step - loss: 0.1873 - sparse_accuracy: 0.9743
test:  f1: 0.81753, precision: 0.80915, recall: 0.82609

Epoch 00031: loss improved from 0.19021 to 0.18802, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 32/300
13/13 [==============================] - 9s 704ms/step - loss: 0.2154 - sparse_accuracy: 0.9730
test:  f1: 0.82297, precision: 0.81517, recall: 0.83092

Epoch 00032: loss did not improve from 0.18802
Epoch 33/300
13/13 [==============================] - 10s 765ms/step - loss: 0.1945 - sparse_accuracy: 0.9721
test:  f1: 0.82830, precision: 0.81790, recall: 0.83897

Epoch 00033: loss did not improve from 0.18802
Epoch 34/300
13/13 [==============================] - 10s 745ms/step - loss: 0.1802 - sparse_accuracy: 0.9746
test:  f1: 0.81594, precision: 0.80757, recall: 0.82448

Epoch 00034: loss improved from 0.18802 to 0.17664, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 35/300
13/13 [==============================] - 11s 825ms/step - loss: 0.1689 - sparse_accuracy: 0.9739
test:  f1: 0.82418, precision: 0.81447, recall: 0.83414

Epoch 00035: loss improved from 0.17664 to 0.16849, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 36/300
13/13 [==============================] - 9s 723ms/step - loss: 0.1746 - sparse_accuracy: 0.9746
test:  f1: 0.82605, precision: 0.81505, recall: 0.83736

Epoch 00036: loss did not improve from 0.16849
Epoch 37/300
13/13 [==============================] - 10s 733ms/step - loss: 0.1893 - sparse_accuracy: 0.9735
test:  f1: 0.81949, precision: 0.81300, recall: 0.82609

Epoch 00037: loss did not improve from 0.16849
Epoch 38/300
13/13 [==============================] - 11s 841ms/step - loss: 0.1745 - sparse_accuracy: 0.9756
test:  f1: 0.81847, precision: 0.80945, recall: 0.82770

Epoch 00038: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.

Epoch 00038: loss did not improve from 0.16849
Epoch 39/300
13/13 [==============================] - 10s 753ms/step - loss: 0.1654 - sparse_accuracy: 0.9768
test:  f1: 0.82709, precision: 0.81861, recall: 0.83575

Epoch 00039: loss improved from 0.16849 to 0.16578, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 40/300
13/13 [==============================] - 10s 749ms/step - loss: 0.1566 - sparse_accuracy: 0.9761
test:  f1: 0.81905, precision: 0.80751, recall: 0.83092

Epoch 00040: loss improved from 0.16578 to 0.15726, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 41/300
13/13 [==============================] - 9s 675ms/step - loss: 0.1630 - sparse_accuracy: 0.9764
test:  f1: 0.81941, precision: 0.80975, recall: 0.82931

Epoch 00041: loss did not improve from 0.15726
Epoch 42/300
13/13 [==============================] - 9s 727ms/step - loss: 0.1475 - sparse_accuracy: 0.9765
test:  f1: 0.82605, precision: 0.81505, recall: 0.83736

Epoch 00042: loss improved from 0.15726 to 0.14832, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 43/300
13/13 [==============================] - 10s 732ms/step - loss: 0.1397 - sparse_accuracy: 0.9799
test:  f1: 0.82288, precision: 0.81191, recall: 0.83414

Epoch 00043: loss improved from 0.14832 to 0.13820, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 44/300
13/13 [==============================] - 10s 738ms/step - loss: 0.1543 - sparse_accuracy: 0.9785
test:  f1: 0.82166, precision: 0.81260, recall: 0.83092

Epoch 00044: loss did not improve from 0.13820
Epoch 45/300
13/13 [==============================] - 9s 730ms/step - loss: 0.1762 - sparse_accuracy: 0.9778
test:  f1: 0.82325, precision: 0.81417, recall: 0.83253

Epoch 00045: loss did not improve from 0.13820
Epoch 46/300
13/13 [==============================] - 9s 720ms/step - loss: 0.1589 - sparse_accuracy: 0.9780
test:  f1: 0.82830, precision: 0.81790, recall: 0.83897

Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.

Epoch 00046: loss did not improve from 0.13820
Epoch 47/300
13/13 [==============================] - 9s 691ms/step - loss: 0.1594 - sparse_accuracy: 0.9766
test:  f1: 0.82962, precision: 0.82047, recall: 0.83897

Epoch 00047: loss did not improve from 0.13820
Epoch 48/300
13/13 [==============================] - 10s 801ms/step - loss: 0.1416 - sparse_accuracy: 0.9785
test:  f1: 0.82418, precision: 0.81447, recall: 0.83414

Epoch 00048: loss did not improve from 0.13820
Epoch 49/300
13/13 [==============================] - 10s 778ms/step - loss: 0.1410 - sparse_accuracy: 0.9797
test:  f1: 0.82643, precision: 0.81732, recall: 0.83575

Epoch 00049: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.

Epoch 00049: loss did not improve from 0.13820
Epoch 50/300
13/13 [==============================] - 11s 836ms/step - loss: 0.1334 - sparse_accuracy: 0.9789
test:  f1: 0.82643, precision: 0.81732, recall: 0.83575

Epoch 00050: loss improved from 0.13820 to 0.13401, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 51/300
13/13 [==============================] - 10s 733ms/step - loss: 0.1365 - sparse_accuracy: 0.9783
test:  f1: 0.82803, precision: 0.81890, recall: 0.83736

Epoch 00051: loss did not improve from 0.13401
Epoch 52/300
13/13 [==============================] - 12s 910ms/step - loss: 0.1421 - sparse_accuracy: 0.9775
test:  f1: 0.82643, precision: 0.81732, recall: 0.83575

Epoch 00052: loss did not improve from 0.13401
Epoch 53/300
13/13 [==============================] - 10s 758ms/step - loss: 0.1370 - sparse_accuracy: 0.9788
test:  f1: 0.82643, precision: 0.81732, recall: 0.83575

Epoch 00053: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.

Epoch 00053: loss did not improve from 0.13401
Epoch 54/300
13/13 [==============================] - 10s 738ms/step - loss: 0.1352 - sparse_accuracy: 0.9794
test:  f1: 0.82643, precision: 0.81732, recall: 0.83575

Epoch 00054: loss improved from 0.13401 to 0.13354, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 55/300
13/13 [==============================] - 10s 767ms/step - loss: 0.1344 - sparse_accuracy: 0.9780
test:  f1: 0.82643, precision: 0.81732, recall: 0.83575

Epoch 00055: loss did not improve from 0.13354
Epoch 56/300
13/13 [==============================] - 11s 812ms/step - loss: 0.1396 - sparse_accuracy: 0.9787
test:  f1: 0.82643, precision: 0.81732, recall: 0.83575

Epoch 00056: loss did not improve from 0.13354
Epoch 57/300
13/13 [==============================] - 10s 762ms/step - loss: 0.1283 - sparse_accuracy: 0.9789
test:  f1: 0.82643, precision: 0.81732, recall: 0.83575

Epoch 00057: loss improved from 0.13354 to 0.12838, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 58/300
13/13 [==============================] - 9s 721ms/step - loss: 0.1291 - sparse_accuracy: 0.9785
test:  f1: 0.82643, precision: 0.81732, recall: 0.83575

Epoch 00058: loss did not improve from 0.12838
Epoch 59/300
13/13 [==============================] - 10s 769ms/step - loss: 0.1337 - sparse_accuracy: 0.9784
test:  f1: 0.82869, precision: 0.82019, recall: 0.83736

Epoch 00059: loss did not improve from 0.12838
Epoch 60/300
13/13 [==============================] - 11s 819ms/step - loss: 0.1372 - sparse_accuracy: 0.9787
test:  f1: 0.82869, precision: 0.82019, recall: 0.83736

Epoch 00060: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.

Epoch 00060: loss did not improve from 0.12838
Epoch 61/300
13/13 [==============================] - 10s 801ms/step - loss: 0.1406 - sparse_accuracy: 0.9786
test:  f1: 0.82869, precision: 0.82019, recall: 0.83736

Epoch 00061: loss did not improve from 0.12838
Epoch 62/300
13/13 [==============================] - 10s 794ms/step - loss: 0.1257 - sparse_accuracy: 0.9798
test:  f1: 0.82643, precision: 0.81732, recall: 0.83575

Epoch 00062: loss improved from 0.12838 to 0.12616, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 63/300
13/13 [==============================] - 9s 690ms/step - loss: 0.1314 - sparse_accuracy: 0.9783
test:  f1: 0.83028, precision: 0.82177, recall: 0.83897

Epoch 00063: loss did not improve from 0.12616
Epoch 64/300
13/13 [==============================] - 11s 814ms/step - loss: 0.1369 - sparse_accuracy: 0.9790
test:  f1: 0.83028, precision: 0.82177, recall: 0.83897

Epoch 00064: loss did not improve from 0.12616
Epoch 65/300
13/13 [==============================] - 10s 785ms/step - loss: 0.1352 - sparse_accuracy: 0.9793
test:  f1: 0.83028, precision: 0.82177, recall: 0.83897

Epoch 00065: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.

Epoch 00065: loss did not improve from 0.12616
Epoch 66/300
13/13 [==============================] - 9s 719ms/step - loss: 0.1313 - sparse_accuracy: 0.9795
test:  f1: 0.83028, precision: 0.82177, recall: 0.83897

Epoch 00066: loss did not improve from 0.12616
Epoch 67/300
13/13 [==============================] - 10s 738ms/step - loss: 0.1238 - sparse_accuracy: 0.9789
test:  f1: 0.82803, precision: 0.81890, recall: 0.83736

Epoch 00067: loss improved from 0.12616 to 0.12495, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 68/300
13/13 [==============================] - 10s 788ms/step - loss: 0.1340 - sparse_accuracy: 0.9789
test:  f1: 0.82643, precision: 0.81732, recall: 0.83575

Epoch 00068: loss did not improve from 0.12495
Epoch 69/300
13/13 [==============================] - 10s 732ms/step - loss: 0.1346 - sparse_accuracy: 0.9795
test:  f1: 0.82643, precision: 0.81732, recall: 0.83575

Epoch 00069: loss did not improve from 0.12495
Epoch 70/300
13/13 [==============================] - 9s 710ms/step - loss: 0.1266 - sparse_accuracy: 0.9797
test:  f1: 0.82643, precision: 0.81732, recall: 0.83575

Epoch 00070: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.

Epoch 00070: loss did not improve from 0.12495
Epoch 71/300
13/13 [==============================] - 11s 827ms/step - loss: 0.1321 - sparse_accuracy: 0.9809
test:  f1: 0.82643, precision: 0.81732, recall: 0.83575

Epoch 00071: loss did not improve from 0.12495
Epoch 72/300
13/13 [==============================] - 11s 812ms/step - loss: 0.1394 - sparse_accuracy: 0.9788
test:  f1: 0.82869, precision: 0.82019, recall: 0.83736

Epoch 00072: loss did not improve from 0.12495
Epoch 73/300
13/13 [==============================] - 10s 748ms/step - loss: 0.1316 - sparse_accuracy: 0.9794
test:  f1: 0.82869, precision: 0.82019, recall: 0.83736

Epoch 00073: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.

Epoch 00073: loss did not improve from 0.12495
Epoch 74/300
13/13 [==============================] - 11s 828ms/step - loss: 0.1214 - sparse_accuracy: 0.9798
test:  f1: 0.82869, precision: 0.82019, recall: 0.83736

Epoch 00074: loss improved from 0.12495 to 0.12383, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 75/300
13/13 [==============================] - 10s 746ms/step - loss: 0.1292 - sparse_accuracy: 0.9789
test:  f1: 0.82869, precision: 0.82019, recall: 0.83736

Epoch 00075: loss did not improve from 0.12383
Epoch 76/300
13/13 [==============================] - 10s 745ms/step - loss: 0.1157 - sparse_accuracy: 0.9806
test:  f1: 0.82869, precision: 0.82019, recall: 0.83736

Epoch 00076: loss improved from 0.12383 to 0.11827, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner_2.h5
Epoch 77/300
13/13 [==============================] - 9s 659ms/step - loss: 0.1464 - sparse_accuracy: 0.9779
test:  f1: 0.82869, precision: 0.82019, recall: 0.83736

Epoch 00077: loss did not improve from 0.11827
Epoch 78/300
13/13 [==============================] - 10s 807ms/step - loss: 0.1349 - sparse_accuracy: 0.9789
test:  f1: 0.82869, precision: 0.82019, recall: 0.83736

Epoch 00078: loss did not improve from 0.11827
Epoch 79/300
13/13 [==============================] - 10s 778ms/step - loss: 0.1331 - sparse_accuracy: 0.9798
test:  f1: 0.82869, precision: 0.82019, recall: 0.83736

Epoch 00079: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.

Epoch 00079: loss did not improve from 0.11827
Epoch 80/300
13/13 [==============================] - 10s 744ms/step - loss: 0.1315 - sparse_accuracy: 0.9788
test:  f1: 0.82869, precision: 0.82019, recall: 0.83736

Epoch 00080: loss did not improve from 0.11827
Epoch 81/300
13/13 [==============================] - 10s 746ms/step - loss: 0.1298 - sparse_accuracy: 0.9792
test:  f1: 0.82643, precision: 0.81732, recall: 0.83575

Epoch 00081: loss did not improve from 0.11827
Epoch 82/300
13/13 [==============================] - 11s 808ms/step - loss: 0.1235 - sparse_accuracy: 0.9789
test:  f1: 0.82643, precision: 0.81732, recall: 0.83575

Epoch 00082: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.

Epoch 00082: loss did not improve from 0.11827
Epoch 83/300
13/13 [==============================] - 10s 791ms/step - loss: 0.1313 - sparse_accuracy: 0.9788
test:  f1: 0.82643, precision: 0.81732, recall: 0.83575

Epoch 00083: loss did not improve from 0.11827
Epoch 84/300
13/13 [==============================] - 10s 746ms/step - loss: 0.1356 - sparse_accuracy: 0.9777
test:  f1: 0.82643, precision: 0.81732, recall: 0.83575

Epoch 00084: loss did not improve from 0.11827
Epoch 85/300
13/13 [==============================] - 11s 808ms/step - loss: 0.1329 - sparse_accuracy: 0.9788
test:  f1: 0.82643, precision: 0.81732, recall: 0.83575

Epoch 00085: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.

Epoch 00085: loss did not improve from 0.11827
Epoch 86/300
13/13 [==============================] - 10s 749ms/step - loss: 0.1194 - sparse_accuracy: 0.9780
test:  f1: 0.82643, precision: 0.81732, recall: 0.83575

Epoch 00086: loss did not improve from 0.11827
Epoch 00086: early stopping

进程已结束,退出代码0
