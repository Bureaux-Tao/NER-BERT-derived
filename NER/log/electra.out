ssh://bureaux@172.30.2.148:22/home/bureaux/miniconda3/envs/Keras-base/bin/python -u /home/bureaux/Projects/keras4bert/NER/train.py
Using TensorFlow backend.
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.

WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.

WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
Input-Token (InputLayer)        (None, None)         0
__________________________________________________________________________________________________
Input-Segment (InputLayer)      (None, None)         0
__________________________________________________________________________________________________
Embedding-Token (Embedding)     (None, None, 768)    16226304    Input-Token[0][0]
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]
                                                                 Embedding-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Position[0][0]
__________________________________________________________________________________________________
Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]
                                                                 Embedding-Dropout[0][0]
                                                                 Embedding-Dropout[0][0]
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]
                                                                 Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward (Feed (None, None, 768)    4722432     Transformer-0-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-0-FeedForward-Dropo (None, None, 768)    0           Transformer-0-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-0-FeedForward-Add ( (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent
                                                                 Transformer-0-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-0-FeedForward-Norm  (None, None, 768)    1536        Transformer-0-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-0-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-FeedForward-Norm[0]
                                                                 Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent
                                                                 Transformer-1-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-1-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]
                                                                 Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent
                                                                 Transformer-2-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-2-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]
                                                                 Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward (Feed (None, None, 768)    4722432     Transformer-3-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent
                                                                 Transformer-3-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-3-FeedForward-Norm  (None, None, 768)    1536        Transformer-3-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-3-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]
                                                                 Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward (Feed (None, None, 768)    4722432     Transformer-4-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent
                                                                 Transformer-4-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-4-FeedForward-Norm  (None, None, 768)    1536        Transformer-4-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-4-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]
                                                                 Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward (Feed (None, None, 768)    4722432     Transformer-5-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent
                                                                 Transformer-5-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-5-FeedForward-Norm  (None, None, 768)    1536        Transformer-5-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-5-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-FeedForward-Norm[0]
                                                                 Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward (Feed (None, None, 768)    4722432     Transformer-6-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-6-FeedForward-Dropo (None, None, 768)    0           Transformer-6-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-6-FeedForward-Add ( (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent
                                                                 Transformer-6-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-6-FeedForward-Norm  (None, None, 768)    1536        Transformer-6-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-6-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-FeedForward-Norm[0]
                                                                 Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward (Feed (None, None, 768)    4722432     Transformer-7-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-7-FeedForward-Dropo (None, None, 768)    0           Transformer-7-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-7-FeedForward-Add ( (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent
                                                                 Transformer-7-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-7-FeedForward-Norm  (None, None, 768)    1536        Transformer-7-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-7-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-FeedForward-Norm[0]
                                                                 Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward (Feed (None, None, 768)    4722432     Transformer-8-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-8-FeedForward-Dropo (None, None, 768)    0           Transformer-8-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-8-FeedForward-Add ( (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent
                                                                 Transformer-8-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-8-FeedForward-Norm  (None, None, 768)    1536        Transformer-8-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-8-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-FeedForward-Norm[0]
                                                                 Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward (Feed (None, None, 768)    4722432     Transformer-9-MultiHeadSelfAttent
__________________________________________________________________________________________________
Transformer-9-FeedForward-Dropo (None, None, 768)    0           Transformer-9-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-9-FeedForward-Add ( (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent
                                                                 Transformer-9-FeedForward-Dropout
__________________________________________________________________________________________________
Transformer-9-FeedForward-Norm  (None, None, 768)    1536        Transformer-9-FeedForward-Add[0][
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-9-FeedForward-Norm[0]
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-9-FeedForward-Norm[0]
                                                                 Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward (Fee (None, None, 768)    4722432     Transformer-10-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-10-FeedForward-Drop (None, None, 768)    0           Transformer-10-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-10-FeedForward-Add  (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten
                                                                 Transformer-10-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-10-FeedForward-Norm (None, None, 768)    1536        Transformer-10-FeedForward-Add[0]
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
                                                                 Transformer-10-FeedForward-Norm[0
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-FeedForward-Norm[0
                                                                 Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward (Fee (None, None, 768)    4722432     Transformer-11-MultiHeadSelfAtten
__________________________________________________________________________________________________
Transformer-11-FeedForward-Drop (None, None, 768)    0           Transformer-11-FeedForward[0][0]
__________________________________________________________________________________________________
Transformer-11-FeedForward-Add  (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten
                                                                 Transformer-11-FeedForward-Dropou
__________________________________________________________________________________________________
Transformer-11-FeedForward-Norm (None, None, 768)    1536        Transformer-11-FeedForward-Add[0]
__________________________________________________________________________________________________
bidirectional_1 (Bidirectional) (None, None, 128)    426496      Transformer-11-FeedForward-Norm[0
__________________________________________________________________________________________________
time_distributed_1 (TimeDistrib (None, None, 29)     3741        bidirectional_1[0][0]
__________________________________________________________________________________________________
conditional_random_field_1 (Con (None, None, 29)     841         time_distributed_1[0][0]
==================================================================================================
Total params: 102,108,134
Trainable params: 102,108,134
Non-trainable params: 0
__________________________________________________________________________________________________
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Keras-base/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

Epoch 1/300
13/13 [==============================] - 33s 3s/step - loss: 33.6423 - sparse_accuracy: 0.4496
test:  f1: 0.55434, precision: 0.58127, recall: 0.52979


Epoch 00001: loss improved from inf to 34.20830, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 2/300
13/13 [==============================] - 18s 1s/step - loss: 8.9856 - sparse_accuracy: 0.7322
test:  f1: 0.72917, precision: 0.72568, recall: 0.73269


Epoch 00002: loss improved from 34.20830 to 8.99382, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 3/300
13/13 [==============================] - 16s 1s/step - loss: 5.4814 - sparse_accuracy: 0.7946
test:  f1: 0.74235, precision: 0.74235, recall: 0.74235


Epoch 00003: loss improved from 8.99382 to 5.48224, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 4/300
13/13 [==============================] - 16s 1s/step - loss: 3.9836 - sparse_accuracy: 0.7949
test:  f1: 0.73882, precision: 0.72018, recall: 0.75845


Epoch 00004: loss improved from 5.48224 to 3.99013, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 5/300
13/13 [==============================] - 17s 1s/step - loss: 3.6782 - sparse_accuracy: 0.8128
test:  f1: 0.80032, precision: 0.79088, recall: 0.80998


Epoch 00005: loss improved from 3.99013 to 3.65987, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 6/300
13/13 [==============================] - 17s 1s/step - loss: 2.6907 - sparse_accuracy: 0.8334
test:  f1: 0.80604, precision: 0.79592, recall: 0.81643


Epoch 00006: loss improved from 3.65987 to 2.67434, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 7/300
13/13 [==============================] - 17s 1s/step - loss: 2.2498 - sparse_accuracy: 0.8598
test:  f1: 0.80607, precision: 0.79905, recall: 0.81320


Epoch 00007: loss improved from 2.67434 to 2.22510, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 8/300
13/13 [==============================] - 17s 1s/step - loss: 1.9250 - sparse_accuracy: 0.8706
test:  f1: 0.79778, precision: 0.78594, recall: 0.80998


Epoch 00008: loss improved from 2.22510 to 1.92630, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 9/300
13/13 [==============================] - 16s 1s/step - loss: 1.6242 - sparse_accuracy: 0.8691
test:  f1: 0.80513, precision: 0.80192, recall: 0.80837


Epoch 00009: loss improved from 1.92630 to 1.62361, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 10/300
13/13 [==============================] - 16s 1s/step - loss: 1.3113 - sparse_accuracy: 0.8695
test:  f1: 0.82474, precision: 0.81250, recall: 0.83736


Epoch 00010: loss improved from 1.62361 to 1.31799, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 11/300
13/13 [==============================] - 16s 1s/step - loss: 1.1863 - sparse_accuracy: 0.8850
test:  f1: 0.81717, precision: 0.80691, recall: 0.82770


Epoch 00011: loss improved from 1.31799 to 1.19422, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 12/300
13/13 [==============================] - 17s 1s/step - loss: 1.0947 - sparse_accuracy: 0.8995
test:  f1: 0.80573, precision: 0.79685, recall: 0.81481


Epoch 00012: loss improved from 1.19422 to 1.06552, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 13/300
13/13 [==============================] - 17s 1s/step - loss: 0.9392 - sparse_accuracy: 0.8930
test:  f1: 0.80573, precision: 0.79685, recall: 0.81481


Epoch 00013: loss improved from 1.06552 to 0.93685, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 14/300
13/13 [==============================] - 16s 1s/step - loss: 0.8802 - sparse_accuracy: 0.9075
test:  f1: 0.80414, precision: 0.79528, recall: 0.81320


Epoch 00014: loss improved from 0.93685 to 0.86242, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 15/300
13/13 [==============================] - 17s 1s/step - loss: 0.9110 - sparse_accuracy: 0.8989
test:  f1: 0.80443, precision: 0.79128, recall: 0.81804


Epoch 00015: loss did not improve from 0.86242
Epoch 16/300
13/13 [==============================] - 16s 1s/step - loss: 2.8187 - sparse_accuracy: 0.8424
test:  f1: 0.79681, precision: 0.78864, recall: 0.80515


Epoch 00016: loss did not improve from 0.86242
Epoch 17/300
13/13 [==============================] - 16s 1s/step - loss: 1.4911 - sparse_accuracy: 0.8692
test:  f1: 0.80769, precision: 0.80383, recall: 0.81159


Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00019999999494757503.

Epoch 00017: loss did not improve from 0.86242
Epoch 18/300
13/13 [==============================] - 16s 1s/step - loss: 1.2545 - sparse_accuracy: 0.8770
test:  f1: 0.82092, precision: 0.80811, recall: 0.83414


Epoch 00018: loss did not improve from 0.86242
Epoch 19/300
13/13 [==============================] - 16s 1s/step - loss: 1.6561 - sparse_accuracy: 0.8402
test:  f1: 0.79403, precision: 0.77573, recall: 0.81320


Epoch 00019: loss did not improve from 0.86242
Epoch 20/300
13/13 [==============================] - 16s 1s/step - loss: 1.3497 - sparse_accuracy: 0.8554
test:  f1: 0.82857, precision: 0.81690, recall: 0.84058


Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-05.

Epoch 00020: loss did not improve from 0.86242
Epoch 21/300
13/13 [==============================] - 17s 1s/step - loss: 0.7486 - sparse_accuracy: 0.8972
test:  f1: 0.83333, precision: 0.82160, recall: 0.84541


Epoch 00021: loss improved from 0.86242 to 0.74819, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 22/300
13/13 [==============================] - 16s 1s/step - loss: 0.5879 - sparse_accuracy: 0.8985
test:  f1: 0.83320, precision: 0.81832, recall: 0.84863


Epoch 00022: loss improved from 0.74819 to 0.59484, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 23/300
13/13 [==============================] - 17s 1s/step - loss: 0.4816 - sparse_accuracy: 0.9052
test:  f1: 0.82502, precision: 0.81153, recall: 0.83897


Epoch 00023: loss improved from 0.59484 to 0.48203, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 24/300
13/13 [==============================] - 16s 1s/step - loss: 0.4552 - sparse_accuracy: 0.9078
test:  f1: 0.82660, precision: 0.81308, recall: 0.84058


Epoch 00024: loss improved from 0.48203 to 0.45936, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 25/300
13/13 [==============================] - 15s 1s/step - loss: 0.4130 - sparse_accuracy: 0.9096
test:  f1: 0.82055, precision: 0.80590, recall: 0.83575


Epoch 00025: loss improved from 0.45936 to 0.41352, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 26/300
13/13 [==============================] - 17s 1s/step - loss: 0.3934 - sparse_accuracy: 0.9121
test:  f1: 0.83016, precision: 0.81847, recall: 0.84219


Epoch 00026: loss improved from 0.41352 to 0.39189, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 27/300
13/13 [==============================] - 16s 1s/step - loss: 0.3633 - sparse_accuracy: 0.9129
test:  f1: 0.83189, precision: 0.81579, recall: 0.84863


Epoch 00027: loss improved from 0.39189 to 0.36090, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 28/300
13/13 [==============================] - 16s 1s/step - loss: 0.3475 - sparse_accuracy: 0.9118
test:  f1: 0.82938, precision: 0.81395, recall: 0.84541


Epoch 00028: loss improved from 0.36090 to 0.34312, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 29/300
13/13 [==============================] - 16s 1s/step - loss: 0.3487 - sparse_accuracy: 0.9146
test:  f1: 0.83189, precision: 0.81579, recall: 0.84863


Epoch 00029: loss did not improve from 0.34312
Epoch 30/300
13/13 [==============================] - 17s 1s/step - loss: 0.3131 - sparse_accuracy: 0.9186
test:  f1: 0.82306, precision: 0.80775, recall: 0.83897


Epoch 00030: loss improved from 0.34312 to 0.31181, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 31/300
13/13 [==============================] - 16s 1s/step - loss: 0.3024 - sparse_accuracy: 0.9201
test:  f1: 0.82427, precision: 0.80710, recall: 0.84219


Epoch 00031: loss improved from 0.31181 to 0.30080, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 32/300
13/13 [==============================] - 17s 1s/step - loss: 0.2781 - sparse_accuracy: 0.9193
test:  f1: 0.82846, precision: 0.81366, recall: 0.84380


Epoch 00032: loss improved from 0.30080 to 0.27906, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 33/300
13/13 [==============================] - 16s 1s/step - loss: 0.2803 - sparse_accuracy: 0.9287
test:  f1: 0.82567, precision: 0.81279, recall: 0.83897


Epoch 00033: loss improved from 0.27906 to 0.27903, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 34/300
13/13 [==============================] - 16s 1s/step - loss: 0.2577 - sparse_accuracy: 0.9269
test:  f1: 0.82950, precision: 0.81719, recall: 0.84219


Epoch 00034: loss improved from 0.27903 to 0.26078, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 35/300
13/13 [==============================] - 15s 1s/step - loss: 0.2654 - sparse_accuracy: 0.9242
test:  f1: 0.82698, precision: 0.81534, recall: 0.83897


Epoch 00035: loss did not improve from 0.26078
Epoch 36/300
13/13 [==============================] - 16s 1s/step - loss: 0.2543 - sparse_accuracy: 0.9281
test:  f1: 0.82567, precision: 0.81279, recall: 0.83897


Epoch 00036: loss improved from 0.26078 to 0.25559, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 37/300
13/13 [==============================] - 16s 1s/step - loss: 0.2438 - sparse_accuracy: 0.9299
test:  f1: 0.83043, precision: 0.81747, recall: 0.84380


Epoch 00037: loss improved from 0.25559 to 0.24683, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 38/300
13/13 [==============================] - 16s 1s/step - loss: 0.2657 - sparse_accuracy: 0.9278
test:  f1: 0.82977, precision: 0.81620, recall: 0.84380


Epoch 00038: loss did not improve from 0.24683
Epoch 39/300
13/13 [==============================] - 17s 1s/step - loss: 0.2403 - sparse_accuracy: 0.9308
test:  f1: 0.82857, precision: 0.81690, recall: 0.84058


Epoch 00039: loss improved from 0.24683 to 0.23964, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 40/300
13/13 [==============================] - 18s 1s/step - loss: 0.2412 - sparse_accuracy: 0.9343
test:  f1: 0.82857, precision: 0.81690, recall: 0.84058


Epoch 00040: loss improved from 0.23964 to 0.23659, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 41/300
13/13 [==============================] - 17s 1s/step - loss: 0.2258 - sparse_accuracy: 0.9328
test:  f1: 0.82595, precision: 0.81182, recall: 0.84058


Epoch 00041: loss improved from 0.23659 to 0.22254, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 42/300
13/13 [==============================] - 17s 1s/step - loss: 0.2295 - sparse_accuracy: 0.9339
test:  f1: 0.82764, precision: 0.81661, recall: 0.83897


Epoch 00042: loss did not improve from 0.22254
Epoch 43/300
13/13 [==============================] - 16s 1s/step - loss: 0.2352 - sparse_accuracy: 0.9287
test:  f1: 0.83584, precision: 0.82344, recall: 0.84863


Epoch 00043: loss did not improve from 0.22254
Epoch 44/300
13/13 [==============================] - 17s 1s/step - loss: 0.2293 - sparse_accuracy: 0.9329
test:  f1: 0.83386, precision: 0.81960, recall: 0.84863


Epoch 00044: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.

Epoch 00044: loss did not improve from 0.22254
Epoch 45/300
13/13 [==============================] - 18s 1s/step - loss: 0.2168 - sparse_accuracy: 0.9320
test:  f1: 0.83043, precision: 0.81747, recall: 0.84380


Epoch 00045: loss improved from 0.22254 to 0.21760, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 46/300
13/13 [==============================] - 16s 1s/step - loss: 0.2012 - sparse_accuracy: 0.9316
test:  f1: 0.83360, precision: 0.82059, recall: 0.84702


Epoch 00046: loss improved from 0.21760 to 0.20297, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 47/300
13/13 [==============================] - 16s 1s/step - loss: 0.2142 - sparse_accuracy: 0.9383
test:  f1: 0.83135, precision: 0.81776, recall: 0.84541


Epoch 00047: loss did not improve from 0.20297
Epoch 48/300
13/13 [==============================] - 16s 1s/step - loss: 0.1998 - sparse_accuracy: 0.9357
test:  f1: 0.82567, precision: 0.81279, recall: 0.83897


Epoch 00048: loss improved from 0.20297 to 0.19745, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 49/300
13/13 [==============================] - 16s 1s/step - loss: 0.2113 - sparse_accuracy: 0.9328
test:  f1: 0.82726, precision: 0.81435, recall: 0.84058


Epoch 00049: loss did not improve from 0.19745
Epoch 50/300
13/13 [==============================] - 16s 1s/step - loss: 0.1967 - sparse_accuracy: 0.9344
test:  f1: 0.82977, precision: 0.81620, recall: 0.84380


Epoch 00050: loss improved from 0.19745 to 0.19671, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 51/300
13/13 [==============================] - 17s 1s/step - loss: 0.1966 - sparse_accuracy: 0.9373
test:  f1: 0.82950, precision: 0.81719, recall: 0.84219


Epoch 00051: loss did not improve from 0.19671
Epoch 52/300
13/13 [==============================] - 16s 1s/step - loss: 0.1845 - sparse_accuracy: 0.9389
test:  f1: 0.83201, precision: 0.81903, recall: 0.84541


Epoch 00052: loss improved from 0.19671 to 0.18561, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 53/300
13/13 [==============================] - 16s 1s/step - loss: 0.1933 - sparse_accuracy: 0.9395
test:  f1: 0.82911, precision: 0.81493, recall: 0.84380


Epoch 00053: loss did not improve from 0.18561
Epoch 54/300
13/13 [==============================] - 16s 1s/step - loss: 0.1943 - sparse_accuracy: 0.9390
test:  f1: 0.82884, precision: 0.81591, recall: 0.84219


Epoch 00054: loss did not improve from 0.18561
Epoch 55/300
13/13 [==============================] - 16s 1s/step - loss: 0.2158 - sparse_accuracy: 0.9375
test:  f1: 0.82819, precision: 0.81464, recall: 0.84219


Epoch 00055: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.

Epoch 00055: loss did not improve from 0.18561
Epoch 56/300
13/13 [==============================] - 16s 1s/step - loss: 0.1844 - sparse_accuracy: 0.9377
test:  f1: 0.82846, precision: 0.81366, recall: 0.84380


Epoch 00056: loss improved from 0.18561 to 0.18430, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 57/300
13/13 [==============================] - 16s 1s/step - loss: 0.1845 - sparse_accuracy: 0.9380
test:  f1: 0.82819, precision: 0.81464, recall: 0.84219


Epoch 00057: loss did not improve from 0.18430
Epoch 58/300
13/13 [==============================] - 17s 1s/step - loss: 0.1743 - sparse_accuracy: 0.9388
test:  f1: 0.82977, precision: 0.81620, recall: 0.84380


Epoch 00058: loss improved from 0.18430 to 0.17240, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 59/300
13/13 [==============================] - 16s 1s/step - loss: 0.1677 - sparse_accuracy: 0.9406
test:  f1: 0.82819, precision: 0.81464, recall: 0.84219


Epoch 00059: loss improved from 0.17240 to 0.16914, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 60/300
13/13 [==============================] - 16s 1s/step - loss: 0.1710 - sparse_accuracy: 0.9429
test:  f1: 0.82819, precision: 0.81464, recall: 0.84219


Epoch 00060: loss did not improve from 0.16914
Epoch 61/300
13/13 [==============================] - 16s 1s/step - loss: 0.1810 - sparse_accuracy: 0.9431
test:  f1: 0.82819, precision: 0.81464, recall: 0.84219


Epoch 00061: loss did not improve from 0.16914
Epoch 62/300
13/13 [==============================] - 15s 1s/step - loss: 0.1699 - sparse_accuracy: 0.9446
test:  f1: 0.82977, precision: 0.81620, recall: 0.84380


Epoch 00062: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.

Epoch 00062: loss did not improve from 0.16914
Epoch 63/300
13/13 [==============================] - 16s 1s/step - loss: 0.1715 - sparse_accuracy: 0.9440
test:  f1: 0.82977, precision: 0.81620, recall: 0.84380


Epoch 00063: loss did not improve from 0.16914
Epoch 64/300
13/13 [==============================] - 16s 1s/step - loss: 0.1719 - sparse_accuracy: 0.9438
test:  f1: 0.82977, precision: 0.81620, recall: 0.84380


Epoch 00064: loss did not improve from 0.16914
Epoch 65/300
13/13 [==============================] - 16s 1s/step - loss: 0.1763 - sparse_accuracy: 0.9434
test:  f1: 0.83109, precision: 0.81875, recall: 0.84380


Epoch 00065: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.

Epoch 00065: loss did not improve from 0.16914
Epoch 66/300
13/13 [==============================] - 16s 1s/step - loss: 0.1749 - sparse_accuracy: 0.9439
test:  f1: 0.83109, precision: 0.81875, recall: 0.84380


Epoch 00066: loss did not improve from 0.16914
Epoch 67/300
13/13 [==============================] - 16s 1s/step - loss: 0.1734 - sparse_accuracy: 0.9435
test:  f1: 0.83109, precision: 0.81875, recall: 0.84380


Epoch 00067: loss did not improve from 0.16914
Epoch 68/300
13/13 [==============================] - 17s 1s/step - loss: 0.1782 - sparse_accuracy: 0.9435
test:  f1: 0.82819, precision: 0.81464, recall: 0.84219


Epoch 00068: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.

Epoch 00068: loss did not improve from 0.16914
Epoch 69/300
13/13 [==============================] - 17s 1s/step - loss: 0.1661 - sparse_accuracy: 0.9419
test:  f1: 0.82977, precision: 0.81620, recall: 0.84380


Epoch 00069: loss improved from 0.16914 to 0.16703, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 70/300
13/13 [==============================] - 16s 1s/step - loss: 0.1635 - sparse_accuracy: 0.9448
test:  f1: 0.82977, precision: 0.81620, recall: 0.84380


Epoch 00070: loss improved from 0.16703 to 0.16410, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 71/300
13/13 [==============================] - 16s 1s/step - loss: 0.1649 - sparse_accuracy: 0.9451
test:  f1: 0.82819, precision: 0.81464, recall: 0.84219


Epoch 00071: loss did not improve from 0.16410
Epoch 72/300
13/13 [==============================] - 16s 1s/step - loss: 0.1718 - sparse_accuracy: 0.9426
test:  f1: 0.82791, precision: 0.81563, recall: 0.84058


Epoch 00072: loss did not improve from 0.16410
Epoch 73/300
13/13 [==============================] - 16s 1s/step - loss: 0.1640 - sparse_accuracy: 0.9432
test:  f1: 0.82950, precision: 0.81719, recall: 0.84219


Epoch 00073: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.

Epoch 00073: loss did not improve from 0.16410
Epoch 74/300
13/13 [==============================] - 17s 1s/step - loss: 0.1717 - sparse_accuracy: 0.9435
test:  f1: 0.82950, precision: 0.81719, recall: 0.84219


Epoch 00074: loss did not improve from 0.16410
Epoch 75/300
13/13 [==============================] - 16s 1s/step - loss: 0.1721 - sparse_accuracy: 0.9438
test:  f1: 0.83109, precision: 0.81875, recall: 0.84380


Epoch 00075: loss did not improve from 0.16410
Epoch 76/300
13/13 [==============================] - 16s 1s/step - loss: 0.1659 - sparse_accuracy: 0.9429
test:  f1: 0.83109, precision: 0.81875, recall: 0.84380


Epoch 00076: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.

Epoch 00076: loss did not improve from 0.16410
Epoch 77/300
13/13 [==============================] - 16s 1s/step - loss: 0.1682 - sparse_accuracy: 0.9436
test:  f1: 0.83109, precision: 0.81875, recall: 0.84380


Epoch 00077: loss did not improve from 0.16410
Epoch 78/300
13/13 [==============================] - 17s 1s/step - loss: 0.1710 - sparse_accuracy: 0.9421
test:  f1: 0.83109, precision: 0.81875, recall: 0.84380


Epoch 00078: loss did not improve from 0.16410
Epoch 79/300
13/13 [==============================] - 16s 1s/step - loss: 0.1653 - sparse_accuracy: 0.9426
test:  f1: 0.82977, precision: 0.81620, recall: 0.84380


Epoch 00079: loss improved from 0.16410 to 0.16343, saving model to /home/bureaux/Projects/keras4bert/weights/pulmonary_electra_ner.h5
Epoch 80/300
13/13 [==============================] - 17s 1s/step - loss: 0.1614 - sparse_accuracy: 0.9435
test:  f1: 0.82977, precision: 0.81620, recall: 0.84380


Epoch 00080: loss did not improve from 0.16343
Epoch 81/300
13/13 [==============================] - 17s 1s/step - loss: 0.1669 - sparse_accuracy: 0.9434
test:  f1: 0.82977, precision: 0.81620, recall: 0.84380


Epoch 00081: loss did not improve from 0.16343
Epoch 82/300
13/13 [==============================] - 16s 1s/step - loss: 0.1703 - sparse_accuracy: 0.9430
test:  f1: 0.82977, precision: 0.81620, recall: 0.84380


Epoch 00082: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.

Epoch 00082: loss did not improve from 0.16343
Epoch 83/300
13/13 [==============================] - 16s 1s/step - loss: 0.1710 - sparse_accuracy: 0.9426
test:  f1: 0.82977, precision: 0.81620, recall: 0.84380


Epoch 00083: loss did not improve from 0.16343
Epoch 84/300
13/13 [==============================] - 16s 1s/step - loss: 0.1671 - sparse_accuracy: 0.9418
test:  f1: 0.82977, precision: 0.81620, recall: 0.84380


Epoch 00084: loss did not improve from 0.16343
Epoch 85/300
13/13 [==============================] - 17s 1s/step - loss: 0.1725 - sparse_accuracy: 0.9450
test:  f1: 0.82977, precision: 0.81620, recall: 0.84380


Epoch 00085: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.

Epoch 00085: loss did not improve from 0.16343
Epoch 86/300
13/13 [==============================] - 15s 1s/step - loss: 0.1714 - sparse_accuracy: 0.9427
test:  f1: 0.82977, precision: 0.81620, recall: 0.84380


Epoch 00086: loss did not improve from 0.16343
Epoch 87/300
13/13 [==============================] - 17s 1s/step - loss: 0.1704 - sparse_accuracy: 0.9436
test:  f1: 0.82977, precision: 0.81620, recall: 0.84380


Epoch 00087: loss did not improve from 0.16343
Epoch 88/300
13/13 [==============================] - 17s 1s/step - loss: 0.1734 - sparse_accuracy: 0.9441
test:  f1: 0.82977, precision: 0.81620, recall: 0.84380


Epoch 00088: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.

Epoch 00088: loss did not improve from 0.16343
Epoch 89/300
13/13 [==============================] - 16s 1s/step - loss: 0.1782 - sparse_accuracy: 0.9408
test:  f1: 0.82977, precision: 0.81620, recall: 0.84380


Epoch 00089: loss did not improve from 0.16343
Epoch 00089: early stopping

进程已结束,退出代码0
